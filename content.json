{"pages":[{"title":"关于我","text":"","link":"/about/index.html"},{"title":"文章分类","text":"","link":"/categories/index.html"},{"title":"标签","text":"","link":"/tags/index.html"}],"posts":[{"title":"Elasticsearch的使用","text":"用数据库可以实现搜索功能，为什么还需要Elasticsearch呢？在使用数据库搜索时，我们更多的是基于精确匹配的搜索，并不支持相关性匹配，与精确匹配相比，相关性匹配更贴近人的思维方式，而Elasticsearch支持相关性匹配。搜索引擎不只是搜索，还有分析，分析数据的能力，是建立在快速的查询上的，而Elasticsearch之所以能够快速查询，是因为Elasticsearch基于倒排索引，对于文档搜索来说，倒排索引在性能和空间上都有更加明显的优势。Elastic的优势还有：支持中文分词插件。 注：Elasticsearch存储数据的方式是文档存储，把对象原原本本地放进去，取出时直接取出。 在Linux上使用docker安装Elasticsearch1、安装DockerDocker要求CentOS系统的内核版本高于3.10.查看你当前的内核版本 1uname -r 若内核版本较低，可升级内核 1sudo yum update 安装Docker 1yum install docker 设置Docker为开机启动 1systemctl enable docker 启动Docker 1systemctl start docker 2、在Docker上部署Elasticsearch2.1、安装Elasticsearch下载Elasticsearch镜像 1docker pull elasticsearch 查看镜像 1docker images 运行Elasticsearch镜像 1docker run -e ES_JAVA_OPTS=&quot;-Xms256m -Xmx256m&quot; -d -p 9200:9200 -p 9300:9300 --name containerName imageId 查看正在运行的容器 1docker ps 在浏览器中打开http://服务器IP地址:9200,如果看到以下信息则说明安装成功 1234567891011121314151617{ \"name\" : \"530dd7820315\", \"cluster_name\" : \"docker-cluster\", \"cluster_uuid\" : \"7O0fjpBJTkmn_axwmZX0RQ\", \"version\" : { \"number\" : \"7.2.0\", \"build_flavor\" : \"default\", \"build_type\" : \"docker\", \"build_hash\" : \"508c38a\", \"build_date\" : \"2019-06-20T15:54:18.811730Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.0.0\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\"} 2.2、修改配置，解决跨域访问问题首先进入到容器中，然后进入到指定目录修改elasticsearch.yml文件。 123docker exec -it elasticsearch /bin/bashcd /usr/share/elasticsearch/config/vi elasticsearch.yml 在elasticsearch.yml的文件末尾加上 12http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot; 修改配置后重启容器即可 1docker restart elasticsearch 2.3、安装ik分词器es自带的分词器对中文分词不是很友好，所以我们下载开源的IK分词器来解决这个问题。首先进入到plugins目录中下载分词器，下载完成后然后解压，在重启es即可 1234cd /usr/share/elasticsearch/plugins/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.2.0/elasticsearch-analysis-ik-7.2.0.zipexitdocker restart elasticsearch 注意：elasticsearch的版本和ik分词器的版本需要保持一致，不然在重启时会失败。可以在这查看所有版本，选择适合自己版本的右键复制链接地址即可。点击这里 3、用Docker安装kibana安装kibana 1docker pull kibana 启动kibana 12docker run --name kibana --link=elasticsearch:test -p 5601:5601 -d kibana:7.2.0docker start kibana 启动之后可以打开浏览器输入http://服务器IP地址:5601可以打开kibana的界面。","link":"/2020/04/04/Elasticsearch/"},{"title":"TCP&amp;UDP","text":"根据因特网协议栈的划分，网络体系结构可以被分为五层：物理层、链路层、网络层、运输层和应用层。 一、网络体系结构1、应用层 作用：应用层是网络应用程序及他们的应用层协议存留的地方。 主要协议：HTTP，SMTP，Telnet和FTP。 交换信息的分组：报文。 2、运输层 作用：在应用程序端点之间传送应用层报文。 主要协议：TCP和UDP。 交换信息的分组：报文段。 3、网络层 作用：将网络层分组从一台主机移动到另一台主机。 主要协议：IP协议和路由选择协议。 交换信息的分组：数据报。 4、链路层 作用：网络层必须依靠链路层的服务。特别是在每个结点，网络层将数据报下传给链路层，链路层沿着路径将数据报传递给下一个结点。在下个结点，链路层将数据报上传给网络层。 主要协议：以太网、WiFi和电缆接入网的DOCSIS协议。 交换信息的分组：帧。 5、物理层虽然链路层的任务是将整个帧从一个网络元素移动到邻近的网络元素，而物理层的任务是将该帧中的一个一个比特从一个结点移动到下一个结点。在这层中的协议仍然是链路相关的，并且进一步与该链路的实际传输媒体相关。 接下来主要介绍两个具有代表性的运输层协议–TCP和UDP。 二、UDPUDP协议全称是用户数据报协议，在网络中它与TCP协议一样用于处理数据包，是一种无连接的协议。在OSI模型中，在第四层——传输层，处于IP协议的上一层。UDP有不提供数据包分组、组装和不能对数据包进行排序的缺点，也就是说，当报文发送之后，是无法得知其是否安全完整到达的。 它有以下几个特点： 1、面向无连接 首先 UDP 是不需要和 TCP一样在发送数据前进行三次握手建立连接的，想发数据就可以开始发送了。并且也只是数据报文的搬运工，不会对数据报文进行任何拆分和拼接操作。 具体来说就是： 在发送端，应用层将数据传递给传输层的 UDP 协议，UDP 只会给数据增加一个 UDP 头标识下是 UDP 协议，然后就传递给网络层了 在接收端，网络层将数据传递给传输层，UDP 只去除 IP 报文头就传递给应用层，不会任何拼接操作2、有单播，多播，广播的功能 UDP 不止支持一对一的传输方式，同样支持一对多，多对多，多对一的方式，也就是说 UDP 提供了单播，多播，广播的功能。 3、UDP是面向报文的 发送方的UDP对应用程序交下来的报文，在添加首部后就向下交付IP层。UDP对应用层交下来的报文，既不合并，也不拆分，而是保留这些报文的边界。因此，应用程序必须选择合适大小的报文 4、不可靠性 首先不可靠性体现在无连接上，通信都不需要建立连接，想发就发，这样的情况肯定不可靠。 并且收到什么数据就传递什么数据，并且也不会备份数据，发送数据也不会关心对方是否已经正确接收到数据了。 再者网络环境时好时坏，但是 UDP 因为没有拥塞控制，一直会以恒定的速度发送数据。即使网络条件不好，也不会对发送速率进行调整。这样实现的弊端就是在网络条件不好的情况下可能会导致丢包，但是优点也很明显，在某些实时性要求高的场景（比如电话会议）就需要使用 UDP 而不是 TCP。 5、头部开销小，传输数据报文时是很高效的。 UDP 头部包含了以下几个数据： 两个十六位的端口号，分别为源端口（可选字段）和目标端口 整个数据报文的长度 整个数据报文的检验和（IPv4 可选 字段），该字段用于发现头部信息和数据中的错误因此 UDP 的头部开销小，只有八字节，相比 TCP 的至少二十字节要少得多，在传输数据报文时是很高效的。 三、TCP当一台计算机想要与另一台计算机通讯时，两台计算机之间的通信需要畅通且可靠，这样才能保证正确收发数据。例如，当你想查看网页或查看电子邮件时，希望完整且按顺序查看网页，而不丢失任何内容。当你下载文件时，希望获得的是完整的文件，而不仅仅是文件的一部分，因为如果数据丢失或乱序，都不是你希望得到的结果，于是就用到了TCP。 TCP协议全称是传输控制协议是一种面向连接的、可靠的、基于字节流的传输层通信协议，由 IETF 的RFC 793定义。TCP 是面向连接的、可靠的流协议。流就是指不间断的数据结构，你可以把它想象成排水管中的水流。 1、TCP连接过程如下图所示，可以看到建立一个TCP连接的过程为（三次握手的过程）: 第一次握手 客户端向服务端发送连接请求报文段。该报文段中包含自身的数据通讯初始序号。请求发送后，客户端便进入 SYN-SENT 状态。 第二次握手 服务端收到连接请求报文段后，如果同意连接，则会发送一个应答，该应答中也会包含自身的数据通讯初始序号，发送完成后便进入 SYN-RECEIVED 状态。 第三次握手 当客户端收到连接同意的应答后，还要向服务端发送一个确认报文。客户端发完这个报文段后便进入 ESTABLISHED 状态，服务端收到这个应答后也进入 ESTABLISHED 状态，此时连接建立成功。 这里可能大家会有个疑惑：为什么 TCP 建立连接需要三次握手，而不是两次？这是因为这是为了防止出现失效的连接请求报文段被服务端接收的情况，从而产生错误。 2、TCP断开链接TCP 是全双工的，在断开连接时两端都需要发送 FIN 和 ACK。 第一次握手 若客户端 A 认为数据发送完成，则它需要向服务端 B 发送连接释放请求。 第二次握手 B 收到连接释放请求后，会告诉应用层要释放 TCP 链接。然后会发送 ACK 包，并进入 CLOSE_WAIT 状态，此时表明 A 到 B 的连接已经释放，不再接收 A 发的数据了。但是因为 TCP 连接是双向的，所以 B 仍旧可以发送数据给 A。 第三次握手 B 如果此时还有没发完的数据会继续发送，完毕后会向 A 发送连接释放请求，然后 B 便进入 LAST-ACK 状态。 第四次握手 A 收到释放请求后，向 B 发送确认应答，此时 A 进入 TIME-WAIT 状态。该状态会持续 2MSL（最大段生存期，指报文段在网络中生存的时间，超时会被抛弃） 时间，若该时间段内没有 B 的重发请求的话，就进入 CLOSED 状态。当 B 收到确认应答后，也便进入 CLOSED 状态。 3、TCP协议的特点 面向连接 面向连接，是指发送数据之前必须在两端建立连接。建立连接的方法是“三次握手”，这样能建立可靠的连接。建立连接，是为数据的可靠传输打下了基础。 仅支持单播传输 每条TCP传输连接只能有两个端点，只能进行点对点的数据传输，不支持多播和广播传输方式。 面向字节流 TCP不像UDP一样那样一个个报文独立地传输，而是在不保留报文边界的情况下以字节流方式进行传输。 可靠传输 对于可靠传输，判断丢包，误码靠的是TCP的段编号以及确认号。TCP为了保证报文传输的可靠，就给每个包一个序号，同时序号也保证了传送到接收端实体的包的按序接收。然后接收端实体对已成功收到的字节发回一个相应的确认(ACK)；如果发送端实体在合理的往返时延(RTT)内未收到确认，那么对应的数据（假设丢失了）将会被重传。 提供拥塞控制 当网络出现拥塞的时候，TCP能够减小向网络注入数据的速率和数量，缓解拥塞。 TCP提供全双工通信 TCP允许通信双方的应用程序在任何时候都能发送数据，因为TCP连接的两端都设有缓存，用来临时存放双向通信的数据。当然，TCP可以立即发送一个数据段，也可以缓存一段时间以便一次发送更多的数据段（最大的数据段大小取决于MSS） 四、TCP和UDP的比较1、对比 UDP TCP 是否连接 无连接 面向连接 是否可靠 不可靠传输，不使用流量控制和拥塞控制 可靠传输，使用流量控制和拥塞控制 连接个数 支持一对一，一对多，多对一和多对多交互通信 只能是一对一通信 传输方式 面向报文 面向字节流 首部开销 首部开销小，仅8字节 首部最小20字节，最大60字节 适用场景 适用于实时应用（IP电话，视频会议，直播等） 适用于要求可靠传输的应用 2、总结 TCP向上层提供面向连接的可靠服务，UDP向上层提供无连接不可靠服务。 虽然UDP并没有TCP传输来的准确，但是也能在很多实时性要求高的地方有所作为。 对数据准确性要求高，速度可以相对较慢的，可以选用TCP。","link":"/2020/04/20/TCP-UDP/"},{"title":"网络层","text":"​ 网络层是OSI参考模型中的第三层，介于传输层和数据链路层之间，它在数据链路层提供的两个相邻端点之间的数据帧的传送功能上，进一步管理网络中的数据通信，将数据设法从源端经过若干个中间节点传送到目的端，从而向运输层提供最基本的端到端的数据传送服务。网络层有两个重要的功能，转发和路由选择，某些计算机网络中，还有第三种重要的网络功能：连接建立（如ATM、帧中继、MPLS）。 转发涉及分组在单一的路由器中从一条入链路到一条出链路的传送。路由选择涉及一个网络的所有路由器，他们经路由选择协议共同交互，以决定分组从源到目的地节点所采用的路径，决定这些路径的算法被称为路由选择算法。转发是指将分组从一个输入链路接口转移到适当的输出链路接口的路由器本地动作；路由选择是指网络范围的过程，以决定分组从源到目的地所采用的端到端路径。 每台路由器具有一张转发表。路由器通过检查到达分组首部字段的值来转发分组，然后使用该值在该路由器的转发表中索引查询，找到该首部值对应的输出链路接口。 转发问题：编址和最长前缀匹配问题 转发表是由路由选择处理器计算和更新的，但转发表的一份影子副本通常会被存放在每个输入端口。有了影子副本，转发决策能在每个输入端口本地做出，无须调用中央路由选择处理器，避免了集中式处理的瓶颈。 路由选择算法决定了插入路由器的转发表中的值。 分组交换机是指一台通用分组交换设备，分组交换机分为链路分组交换机和路由器，链路分组交换机基于链路层字段中的值做转发决定。路由器基于网络层字段的值做转发决定。 网络层提供了一种单一的服务，尽力而为服务。 路由器的4个组成部分：输入端口、交换结构（核心部位）、输出端口、路由选择处理器 输入端口处理： 交换结构：三种交换技术：（1）经内存交换；（2）经总线交换；（3）经互联网络交换。 输出端口处理： 路由器缓存吸收流量负债的波动，需要的缓存数量应当等于平均往返时延乘以链路的容量。 排队引发的性能，丢包问题（稍后深入了解） 路由选择处理器： 通过路由选择算法进行路由处理，线路分发。 路由选择控制平面因特网的网络层三个主要组件：IP协议；路由选择部分；互联网控制报文协议。通过这些组件，网络层可以在复杂的网络中寻找到最合适的路径，将分组从源主机移动到目的主机。 IPv4：首部检验和的计算：将首部中每两个字节作为一个数，用反码运算对这些数字求和，该和的反码存放在检验和字段，被称为因特网检验和。如何数据包首部中携带的检验和与计算所得不一致，则认为检测出现差错。 为什么运输层和网络层都执行了差错检验？网络层只对IP首部计算检验和，而运输层是对整个报文段计算的。 TCP/UDP与IP不一定必须属于同一个协议栈，即TCP不一定运行在IP上 一些名词解释： 子网：互联某些主机与一个路由器接口的网络形成一个子网 子网掩码：IP编址为子网分配一个地址，如：223.1.1.0/24其中/24记法有时称为子网掩码，，指示了32比特中左侧24比特定义的子网地址，和要连接到223.1.1.0/24的网络主机地址都是233.1.1.xxx的形式 因特网地址分配策略被称为无类别域间路由选择CIDR IP地址是由因特网名字和编号分配机构ICANN管理分配，而一台主机的IP地址通常由动态主机配置协议DHCP来配置。DHCP具有将一个主机连接进一个网络的网络相关方面的自动能力 管理IP地址的典型方法：网络地址转换NAT NAT能使路由器对于外部世界看起来像一个单一IP的单一设备，使路由器对外界隐藏内部网络的细节。所有离开此内部网络的报文与进入此网络的报文都有一个相同的源地址与目的地址，NAT路由器通过使用一张NAT转换表来区分内部网络中的各个主机，转换表包含端口号与其IP地址。NAT转换表与某台主机中端口号与进程ID对照表类似。 NAT虽然得到广泛应用，但很多人反对NAT，原因如下： 1.端口号是用于进程编址，而不是主机编址 2.路由器通常仅应当处理高达第三层的分组 3.主机应彼此直接对话，结点不应该介入修改IP地址与端口号 4.应该使用IPv6来解决IP地址不足的问题 NAT的另一个问题是妨碍了P2P应用程序， 使得在某个NAT下的主机无法与另一个主机建立对等方发起的一条TCP连接。 UPnP：允许外部主机使用TCP或UDP向NAT化的主机发起通信会话。 IP地址路由过程1、当IP数据包到达主机所在局域网的路由器后，路由器会根据包头查看目的IP地址。 2、路由器拿着目的IP地址和自己的路由表分析是否在同一网络中。 3、如果在同一网络中，则将该IP数据包进行广播，如果没有则将IP数据包发送给网关，有网关分析处理进行发送。 IP地址和MAC地址IP地址和MAC地址的区别IP地址：全局唯一性，使用软件来实现网络中地址管理 MAC地址：本地唯一性，使用硬件实现 有了IP地址为什么还要MAC地址1、IP地址一般情况下容易修改和变动，具有随意性，不能再网络上固定标识一台设备。 2、MAC地址一般情况出厂时有厂家烧录到网卡中，不容易被修改，在局域范围内容易唯一定位一台设备。 3、从拓扑结构和分层上分析，IP地址属于网络层，主要功能在广域网范围内路由寻址，选择醉解路由，而MAC地址在数据链路层，要形成适合于网络媒体上传输的数据帧。","link":"/2020/04/23/network-layer/"},{"title":"了解MVVM，从Vue实例出发","text":"MVVM的由来在过去的几年中，我们已经把很多传统的服务端代码放到了浏览器中，这样就产生了成千上万行的javascript代码，它们连接了各式各样的HTML 和CSS文件，但缺乏正规的组织形式，这也就是为什么越来越多的开发者使用javascript框架。比如：angular、react、vue。浏览器的兼容性问题已经不再是前端的阻碍。前端的项目越来越大，项目的可维护性和扩展性、安全性等成了主要问题。当年为了解决浏览器兼容性问题，出现了很多类库，其中最典型的就是jquery。但是这类库没有实现对业务逻辑的分成，所以维护性和扩展性极差。综上两方面原因，才有了MVVM模式一类框架的出现。比如vue,通过数据的双向绑定，极大了提高了开发效率。 MVVM的概述MVVM是Model-View-ViewModel的简写，它是一种前端视图层的分层开发思想，Model指的是传递的数据，View指的是页面的结构，而ViewModel是MVVM模式的核心，它是连接View和Model的桥梁，当数据变化时ViewModel能够监听到，然后使对应的视图做出自动更新，当视图变化使ViewModel也能监听到，从而使数据自动更新，这就实现了数据的双向绑定，这也是MVVM思想的好处。 采用MVVM模式的框架Vue Vue是一套用于构建用户界面的渐进式JavaScript框架，只关注视图层，方便与第三方库或既有项目整合 代码实例： 12345678910111213141516171819202122232425262728293031&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;meta name=\"viewport\" content=\"width=device-width,initial-scale=1.0\"&gt; &lt;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"&gt; &lt;title&gt;Document&lt;/title&gt; &lt;!--导入Vue的包--&gt; &lt;script src=\"./bin/vue-2.4.0.js\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;!--Vue实例所控制的这个元素区域，就是我们的V--&gt; &lt;div id=\"app\"&gt; &lt;p&gt;{{msg}}&lt;/p&gt;&lt;/div&gt; &lt;script&gt; //创建一个Vue实例 //当我们导入包之后，在浏览器的内存中，就多了一个Vue构造函数 //new出来的这个vm对象，就是我们MVVM中的VM调度者 var vm = new Vue({ el: '#app', //表示当前new的这个Vue实例，要控制页面上的哪个区域 //这里的data就是MVVM中的M，专门用来存放数据 data: { //data属性中，存放的是el要用到的数据 msg: 'HelloWorld' //通过Vue提供的指令，很方便的就能把数据渲染到页面上，程序员不再手动操作DOM元素了 } }） &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;","link":"/2020/03/30/mvvm/"},{"title":"Java基础知识点","text":"Java基础1、Java面向对象编程的三大特性：封装、继承、多态注意：继承时父类中的私有属性和方法，子类无法访问，只是拥有。 2、String、StringBuffer和StringBuilder的区别是什么？ 可变性String类中使用final关键字修饰字符数组来保存字符串，所以String对象是不可变的。StringBuffer和StringBuilder都继承自AbstactStringBuilder类，也是使用字符数组保存字符串，但是没有用final关键字修饰，所以这两种对象是可变的。 线程安全性String中的对象是不可变的，可以理解为常量，线程安全。StringBuffer对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。StringBuilder并没有对方法进行加同步锁，所以是非线程安全的。 性能每次对String类型进行改变时，都会生成一个新的String对象，然后将指针指向新的String对象。StringBuffer每次都会对StringBuffer对象本身进行操作，而不是生成新的对象并改变对象引用。相同情况下使用StringBuilder相比使用StringBuffer仅能获得10%~15%左右的性能提升，但要冒多线程不安全的风险。 3、在一个静态方法内调用一个非静态成员为什么是非法的？由于静态方法可以不通过对象进行调用，因此在静态方法里，不能调用其他非静态变量，也不可以访问非静态变量成员。 4、接口与抽象类的区别？接口的方法默认是public（Java9开始允许定义私有方法），所有方法在接口中不能有实现（Java8开始接口方法可以有默认实现），而抽象类可以有非抽象的方法。接口中除了static、final变量，不能有其他变量，而抽象类中则不一定。一个类可以实现多个接口，但只能实现一个抽象类。接口自己本身可以通过extends关键字扩展多个接口。 5、==与equals==：基本数据类型比较的是值，引用数据类型比较的是内存地址。equals()：(1)类没有覆盖equals()方法。则比较的是内存地址。(2)类覆盖了equals()方法，比较的是内容是否相等，注意：String中的equals方法是被重写过的，比较的是对象的值。当创建String类型的对象时，虚拟机会在常量池中查找有没有已经存在的值和要创建的值相同的对象，如果有就把它赋给当前引用。如果没有就在常量池中重新创建一个String对象。 6、既然有了字节流，为什么还要有字符流？不管是文件读写还是网络发送接收，信息的最小存储单元都是字节，那么为什么I/O流操作要分为字节流操作和字符流操作呢？字符流是由Java虚拟机将字节转换得到的，问题就出在这个过程还算是非常耗时，并且，如果我们不知道编码类型就很容易出现乱码问题。所以，I/O流就干脆提供了一个直接操作字符的接口，方便我们平时对字符进行流操作。 7、BIO，NIO，AIO有什么区别？ BIO：同步阻塞I/O模式，数据的读取写入必须阻塞在一个线程内等待其完成。在活动连接数不是特别高（小于单机1000）的情况下，这种模型是比较不错的，可以让每一个连接专注于自己的I/O并且编程模型简单，也不用过多考虑系统的过载、限流等问题。线程池本身就是一个天然的漏斗，可以缓冲一些系统处理不了的连接或请求。但是，在面对十万甚至百万级连接时，传统的BIO模型是无能为力的。 NIO：NIO是一种同步非阻塞的I/O模型，在Java1.4中引入NIO框架，对应java.nio包，提供了Channel，Selector，Buffer等抽象。NIO中的N可以理解为Non-blocking，不单纯是New。它支持面向缓冲的，基于通道的I/O操作方法。NIO提供了与传统BIO模型中的Socket和ServerSocket相对应的SocketChannel和ServerSocketChannel两种不同的套接字通道实现，两种通道都支持阻塞和非阻塞两种模式，阻塞模式使用就像传统中的支持一样，比较简单，但是性能和可靠性都不好；非阻塞模式正好与之相反。对于低负载、低并发的应用程序，可以使用同步阻塞I/O来提升开发速率和更好的维护性，对于高负载、高并发的(网络)应用，应使用NIO的非阻塞模式来开发 AIO：AIO也就是NIO2。在Java7中引入了NIO的改进版NIO2，它是异步非阻塞的IO模型。异步IO是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会阻塞在哪里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。AIO是异步IO的错写，虽然NIO在网络操作中，提供了非阻塞的方法，但NIO的IO行为还是同步的。对于NIO来说，我们的业务线程是在IO操作准备好是，得到通知，接着就由这个线程自行进行IO操作，IO操作本身是同步的。目前AIO的应用还不是很广泛。 Java集合1、List,Set,Map的区别？List：不唯一，有序Set：唯一，无序Map：使用键值对存储，Map会维护与Key有关联的值。两个Key可以引用相同的对象，但Key不能重复，典型的Key是String类型，但也可以任何对象。 2、ArrayList与LinkedList的区别？线程安全：ArrayList和LinkedList都是不同步的，也就是不保证线程安全；底层数据结构：ArrayList底层使用的是Object数组，LinkedList底层使用的是双向链表；增删数据：LinkedList插入与删除比ArrayList方便；数据快速随机访问：ArrayList支持快速随机访问，LinkedList不支持；（RandomAccess接口作为一个标识，标识实现这个接口的类具有随机访问功能）内存控件占用：ArrayList的空间浪费主要体现在List列表的结尾会预留一定的容量空间，而LinkedList的空间花费则体现在它的每一个元素都需要消耗比ArrayList更多的空间。 3、ArrayList和Vector的区别？Vector类的所有方法都是同步的，可以有两个线程安全地访问一个Vector对象，但是一个线程访问Vector的话，代码要在同步操作上耗费大量的时间；ArrayList不是同步的，所以不需要保证线程安全时建议使用ArrayList。 4、HashMap和HashTable的区别？ 线程安全：HashMap是非线程安全的，如果需要满足线程安全，可以用Collections的synchronizedMap方法是HashMap具有线程安全的能力，或者使用ConcurrentHashMap；HashTable是线程安全的，HashTable内部的方法基本都经过synchronized修饰。 效率：因为线程安全的问题，HashMap要比HashTable效率高一点。另外，HashTable基本被淘汰，不要再代码中使用它。 对Null key和Null value的支持：HashMap中，null可以作为键，这样的键只有一个，可以有一个或多个键所对应的值为null。但是在HashTable中put进的键值只要有一个null，直接抛出NullPointerException。 初始容量大小和每次扩充容量大小的不同：创建时如果不指定容量初始值，HashTable默认的初始大小为11，之后每次扩充，容量变为原来的2n+1。HashMap默认的初始化大小为16，之后每次扩充，容量变为原来的2倍。创建时如果给定了容量初始值，那么HashTable会直接使用你给定的大小，而HashMap会将其扩充为2的幂次方大小（HashMap中的tableSizeFor（）方法保证，下面给出了源代码）。也就是说HashMap总是使用2的幂作为哈希表的大小，后面会介绍到为什么是2的幂次方。 底层数据结构：JDK1.8以后的HashMap在解决哈希表冲突时有了较大的变化，当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间。HashTable没有这样的机制。 5、HashMap和HashSet的区别？HashMap：实现了Map接口，存储键值对，调用put()向map中添加元素，HashMap使用键（Key）计算Hashcode;HashSet:实现Set接口，仅存储对象，调用add()方法向Set中添加元素，HashSet使用成员对象来计算hashcode值，对于两个对象来说hashcode可能相同，所以equals()方法用来判断对象的相等性。 6、HashMap的底层实现JDK1.8之前HashMap底层是数组和链表结合在一起使用也就是链表散列。HashMap通过key的HashCode经过扰动函数处理过后得到hash值，然后通过（n-1)&amp;hash判断当前元素存放的位置（这里的n指的是数组的长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的hash值以及key是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲入。所谓扰动函数指的就是HashMap的hash方法。使用hash方法也就是扰动函数是为了防止一些实现比较差的hashCode（）方法换句话说使用扰动函数之后可以减少碰撞。所谓“拉链法”就是：将链表和数组相结合，也就是说创建一个链表数组，数组中每一个就是一个链表，若遇到哈希冲突，则将冲突的指加到链表中即可。（解决哈希冲突）TreeMap、TreeSet以及JDK1.8之后的HashMap底层都用到了红黑树，红黑数就是为了解决二叉查找数的缺陷，因为二叉查找树在某些情况下会退化成一个线性结构。 7、comparable和Comparator的区别？comparable接口实际上是出自java.lang包，它有一个compareTo(Object obj)方法用来排序comparator接口实际上是出自java.util包，它有一个compare(Object obj1,Object obj2)方法用来排序一般我们需要对一个集合使用自定义排序时，我们就要重写compareTo()方法或compare()方法，当我们需要对某一个集合实现两种排序方式，我们可以重写compareTo()方法和使用自制的Comparator方法或者以两个Comparator来分别实现两种排序 8、为什么要有hashCode（散列码）?我们先以“HashSet如何检查重复”为例子来说明为什么要有hashCode：当你把对象加入HashSet时，HashSet会先计算对象的hashcode值来判断对象加入的位置，同时也会与该位置其他已经加入的对象的hashcode值作比较，如果没有相符的hashcode，HashSet会假设对象没有重复出现。但是如果发现有相同hashcode值的对象，这时会调用equals()方法来检查hashcode相等的对象是否真的相同。如果两者相同HashSet就不会让其加入操作成功，如果不同的话，就会重新散列到其他位置，这样我们就大大减少了equals的次数，相应就大大提高了执行速度。 9、为什么Java中只有值传递？按值调用表示方法接受的是调用者提供的值，而按引用调用表示方法接收的是调用者提供的变量地址。一个方法可以修改传递引用所对应的变量值，而不能修改传递值调用所对应的变量值。Java程序设计语言总是采用按值调用，也就是说，方法得到的是所有参数值的一个拷贝，也就是说，方法不能修改传递给它的任何参数变量的内容。 10、如何选用集合？需要根据键值获取到元素值时就选用Map接口下的集合，需要排序时选择TreeMap，不需要排序时就选择HashMap，需要保证线程安全就选用ConcurrentHashMap，当我们只需要存放元素值时，就选择实现Collection接口的集合，需要保证元素唯一时选择实现Set接口的集合比如TreeSet或HashSet，不需要就选择实现List接口的比如ArrayList或LinkedList，然后再根据实现这些接口的集合的特点来选用。 多线程1、何为线程？线程是比进程更小的执行单位，一个进程在其执行的过程中可以产生多个线程。与进程不同的是同类的多个线程共享进程的堆和方法区资源，但每个线程有自己的程序计数器、虚拟机栈和本地方法栈，所以系统在产生一个线程，或是在各个线程之间切换工作时，负担要比进程小得多，因此，线程也被成为轻量级进程。Java程序天生就是多线程程序，我们可以通过JMX来看一下一个普通的Java程序有哪些线程，代码如下： 123456789101112public class MultiThread{ public static void main(String[] args){ //获取Java线程管理MXBean ThreadMXBean threadMXBean = ManagementFactory.getThreadMXBean(); //不需要获取同步的monitor和synchronizer信息，仅获取线程和线程堆栈信息 ThreadInfo[] threadInfo = threadMXBean.dumpAllThreads(false,false); //遍历线程信息，仅打印线程ID和线程名称信息 for(ThreadInfo threadInfo:threadInfos){ System.out.println(\"[\"+threadInfo.getThreadId()+\"]\"+threadInfo.getThreadName()); } }} 上述程序输出如下：[5] Attach Listener //添加事件[4] Signal Dispatcher //分发处理给JVM信号的线程[3] Finalizer //调用对象finalize方法的线程[2] Reference Handler //清除reference线程[1] main //main线程，程序入口从上面的输出内容可以看出：一个Java程序的运行是main线程和多个其他线程同时运行。 2、产生线程死锁须具备的四个条件 互斥条件：该资源任意一个时刻只由一个线程占用。 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的线程保持不放。 不剥夺条件：线程已获得的资源在未使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才能释放资源。 循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。 注：避免线程死锁只要破坏产生死锁的四个条件之一即可。 3、sleep()方法和wait()方法的区别和共同点？ 两者最主要的区别在于：sleep方法没有释放锁，而wait方法释放了锁。 wait()通常被用于线程间交互/通信，sleep通常被用于暂停执行。 wait()方法被调用后，线程不会自动苏醒，需要别的线程调用同一个对象上的notify()或者notifyAll()方法。sleep()方法执行完成后，线程会自动苏醒。或者可以使用wait(long timeout)超时后线程会自动苏醒。 两者都可以暂停线程的执行。 4、为什么调用start()时会执行run()方法，为什么不能直接调用run()方法？new一个Thread，线程就进入了新建状态；调用start()方法，会启动一个线程并使线程进入了就绪状态，当分配到时间片后就可以开始运行了。start()会执行线程的相应准备工作，然后自动执行run()方法的内容，这是真正的多线程工作。而直接执行run()方法，会把run()方法当成一个main线程下的普通方法去执行，并不会在某个线程中执行它，所以这并不是多线程工作。 5、synchronized关键字synchronized关键字解决的是多个线程之间访问资源的同步性，synchronized关键字可以保证被它修饰的方法或者代码块在任意时刻只能由一个线程执行。 synchronized关键字最主要的三种使用方式： 修饰实例方法：作用于当前对象实例加锁，进入同步代码前要获得当前对象实例的锁。 修饰静态方法：也就是给当前类加锁，会作用于类的所有对象实例，因为静态成员不属于任何一个实例对象，是类成愿（static表明这是该类的一个静态资源，不管new了多少个对象，只有一份）。所以如果一个线程A调用一个实例对象的非静态synchronized方法，而线程B需要调用这个实例对象所属类的静态synchronized方法，是允许的，不会发生互斥现象，因为访问静态synchronized方法占用的锁是当前类的锁，而访问非静态synchronized方法占用的锁是当前实例对象锁。 修饰代码快：指定加锁对象，对给定对象加锁，进入同步代码库前要获得给定对象的锁。","link":"/2020/04/07/summary/"},{"title":"Spring MVC回顾","text":"Spring框架提供了构建Web应用程序的全功能MVC模块–Spring MVC。Spring MVC框架提供了一个DispatcherServlet作用前端控制器来分派请求，同时提供灵活的配置处理程序映射、视图解析、语言环境和主题解析，并支持文件上传。Spring MVC还包含多种视图技术。Spring MVC分离了控制器、模型对象、分派器以及处理程序对象的角色，这种分离让它们更容易进行定制。 Spring MVC的特点 Spring MVC拥有强大的灵活性、非侵入性和可配置性。 Spring MVC提供了一个前端控制器DispatcherServlet，开发者无须额外开发控制器对象。 Spring MVC分工明确，包括控制器、验证器、命令对象、模型对象、处理程序映射视图解析器等等，每一个功能实现由一个专门的对象负责完成。 Spring MVC可以自动绑定用户输入，并正确地转换数据类型。例如，Spring MVC能自动解析字符串，并将其设置为模型的int或float类型的属性。 Spring MVC使用一个名称/值的Map对象实现更加灵活的模型数据传输。 Spring MVC内置了常见的校验器，可以校验用户输入，如果校验不通过，则重定向回输入表单。输入校验是可选的，并且支持编程方式及声明方式。 Spring MVC支持国际化，支持根据用户区域显示多国语言。 一个简单的Spring MVC应用1、引入maven依赖1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;4.2.0.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 2、配置DispatcherServlet许多的MVC框架中，都包含一个用于调度控制的Servlet。Spring MVC也提供了一个前端控制器DispatcherServlet，所有的请求驱动都围绕这个DispatcherServlet来分派请求。DispatcherServlet是一个Servlet(它继承自HttpServlet基类)，因此使用时需要把它配置在Web应用的部署描述符web.xml文件当中，配置信息如下： 123456789101112131415161718192021&lt;servlet&gt; &lt;!--Servlet的名称--&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;!--Servlet对应的java类--&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!--当前Servlet的参数信息--&gt; &lt;init-param&gt; &lt;!--contextConfigLocation是参数名称，该参数的值包含Spring MVC的配置文件路径--&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;/WEB-INF/springmvc-config.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;!--在Web应用启动时立即加载Servlet--&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;!--Servlet映射声明--&gt;&lt;servlet-mapping&gt; &lt;!--请求对应的Servlet的名称--&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;!--监听当前域的所有请求--&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 配置了一个DispatcherServlet，该Servlet在Web应用程序启动时立即加载，DispatcherServlet加载时会需要一个Spring MVC的配置文件，默认情况下，应用会去应用程序文件夹的WEB-INF下查找对应的[servlet-name]-servlet.xml文件，例如本例的是springmvc，默认查找的就是/WEB-INF/springmvc-servlet.xml。本例中通过init-param元素的描述，将查找的Spring MVC配置文件位置修改为/WEB-INF/springmvc-config.xml，解析该文件内容并根据文件配置信息创建一个WebApplicationContext容器对象，也称为上下文环境。WebApplication继承自ApplicationContext容器，它的初始化方式和BeanFactory、ApplicationContext有所区别，因为WebApplicationContext需要ServletContext实例，它必须在拥有Web容器的前提下才能完成启动Spring Web应用上下文的工作。 3、Controller类的实现12345678910111213141516171819202122package com.jenson.controller;import org.apache.commons.logging.Log;import org.apache.commons.logging.LogFactory;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.servlet.ModelAndView;@Controllerpublic class HelloController{ private static final Log logger = LogFactory.getLog(HelloController.class); @RequestMapping(value=\"/hello\") public ModelAndView hello(){ logger.info(\"hello方法被调用\"); //创建准备返回的ModelAndView对象，该对象通常包含了返回视图名、模型的名称以及模型对象 ModelAndView mv = new ModelAndView(); //添加模型数据，可以是任意的POJO对象 mv.setViewName(\"/WEB-INF/content/welcome.jsp\"); //返回ModelAndView对象 return mv; }} 4、配置Spring MVC的Controller本例的配置文件位置在/WEB-INF/springmvc-config.xml。 1234567891011121314&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd\"&gt; &lt;!--配置Handle，映射“/hello”请求--&gt; &lt;bean name=\"/hello\" class=\"com.jenson.controller.HelloController\"/&gt; &lt;!--处理映射器将bean的name作为url进行查找，需要在配置Handle时指定name(即url)--&gt; &lt;bean class=\"org.springframework.web.servlet.handler.BeanNameUrlHandlerMapping\"/&gt; &lt;!--SimpleControllerHandlerAdapter是一个处理器适配器，所有处理适配器都要实现HandlerAdapter接口--&gt; &lt;bean class=\"org.springframework.web.servlet.mvc.SimpleControllerHandlerAdapter\"/&gt; &lt;!--视图解析器--&gt; &lt;bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\"/&gt;&lt;/beans&gt; 5、View页面该应用包含一个视图页面welcome.jsp，用来显示欢迎信息。 12345678910111213&lt;%@page language=\"java\" contentType=\"text/html;charset=UTF-8\" pageEncoding=\"UTF-8\"%&gt;&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\"&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv=\"Content-Type\" content=\"text/html;charset=UTF-8\"&gt;&lt;title&gt;welcome&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;!-- 页面可以访问Controller传递出来的message --&gt;${requestScope.message}&lt;/body&gt;&lt;/html&gt; 详解DispatcherServlet在一个Spring MVC应用运行过程中，前端控制器DispatcherServlet具体的作用是什么呢？分析DispatcherServlet源代码如下： 1234567891011protected void initStrategies(ApplicationContext context){ initMultipartResolver(context); //初始化上传文件解析器 initLocaleResolver(context); //初始化本地化解析器 initThemeResolver(context); //初始化主题解析器 initHandlerMappings(context); //初始化处理器映射器，将请求映射到处理器 initHandlerAdapters(context); //初始化处理器适配器 initHandlerExceptionResolvers(context); //初始化处理器异常解析器，如果执行过程中遇到异常将交给HandlerExceptionResolver来解析 initRequestToViewNameTranslator(context); //初始化请求到视图名称解析器 initViewResolvers(context); //初始化视图解析器，通过ViewResolver解析逻辑视图名到具体视图实现 initFlashMapManager(context); //初始化flash映射管理器} initStrategies方法将在WebApplicationContext初始化后自动执行，自动扫描上下文的Bean，根据名称或类型匹配的机制查找自定义组件，如果没有找到则会装配一套Spring的默认组件。","link":"/2020/04/07/springmvc/"},{"title":"VsCode的使用","text":"VsCode的安装VsCode的扩展插件","link":"/2020/05/07/vscode/"},{"title":"API文档生成工具Swagger2的使用","text":"在前后端分离的开发模式下，前后端系统通过接口进行交互，API接口文档变成了前后端开发人员联系的纽带，变得越来越重要，因此许多的API接口文档自动生成工具开始进入我们的视野。Swagger2是一个规范和完整的框架，用于生成、描述、调用和可视化RESTful风格的Web服务。它能够实时同步api与文档，但它的代码侵入性比较强，会影响正常代码阅读。 SpringBoot集成Swagger21、在pom.xml中添加依赖12345678910&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt; 2、Swagger2配置类12345678910111213141516171819202122@Configuration@EnableSwagger2public class Swagger2Config{ @Bean public Docket createRestApi() { return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()) .select() .apis(RequestHandlerSelectors.basePackage(\"com.jenson.ecommerce\")) .paths(PathSelectors.any()) .build(); } private ApiInfo apiInfo() { return new ApiInfoBuilder() .title(\"标题\") .description(\"相关描述\") .contact(new Contact(\"name\",\"url\",\"email\")) .version(\"1.0\") .build(); }} 3、在开发中使用相关注解(1)@Api使用在Controller层Api类上，主要属性有tags(标签)、hidden(是否隐藏)、value、authorizations等。 (2)@ApiOperation使用在Api类的接口方法上，主要属性有value(接口名称)、notes(注释)、hidden(是否隐藏)、httpMethod、ignoreJsonView、response、responseHeaders等等，某些属性注解可自动识别，无需配置。 (3)@ApiImplicitParams、@ApiImplicitParam使用在Api类的接口方法上，对接口参数进行说明，@ApiImplicitParams只有一个属性value，@ApiImplicitParam主要属性有name(参数名称)、value(参数说明)、required(是否必需)、dataType(数据类型)、paramType(参数类型)、dataTypeClass、defaultValue、readOnly等。 (4)@ApiModel用在实体类上，主要属性有description(描述)、parent(父类)、subTypes、value、discriminator等。 (5)@ApiModelProperty用在实体类属性上，主要属性有access、accessMode、allowableValues、allowEmptyValue(是否允许为空)、dataType(数据类型)、example(示例)、hidden(是否隐藏)、name(名称)、notes、required(是否必需)、value(说明)等。 注意：要保证实体类属性都有相应的get、set方法，否则swagger-ui页面无该属性说明。 4、打开swagger-ui界面运行项目后，登录localhost:项目端口号/swagger-ui.html。","link":"/2020/04/03/swagger2/"},{"title":"Webpack","text":"Webpack是一个现代JavaScript应用程序的静态模块打包器，当Webpack处理应用程序时，它会递归地构建一个依赖关系图，其中包含应用程序需要的每个模块，然后将所有这些模块打包成一个或多个bundle。 为什么要使用Webpack​ 以前的前端，很多静态资源、CSS、图片和JS都是手动引入HTML页面中，杂乱无章的代码混在一个文件中，想要寻找某个功能的代码很是困难，要是分开多个文件引入，又会造成HTTP请求数过多的问题。 ​ 为了解决这个问题，出现了许多模块化工具，其中，Webpack具有模块化和组件化的特性，它将一个项目看做一个整体，简化了开发的复杂度，提高了我们的开发效率。 除此之外，Webpack在资源处理方面具有一定优势，它通过代码拆分来做资源异步加载，会消除对未引用资源的依赖，能够控制资源的处理方式，从而加快处理速度。 Webpack的使用初步了解了Webpack的优势后，接着了解如何使用Webpack。 安装Webpack可以使用npm安装，新建一个项目，在终端中转到该项目所在位置后执行下述指令就可以完成安装。 1234//全局安装npm install -g webpack//安装到项目目录npm install --save--dev webpack 使用Webpack的准备工作1.在项目中创建一个package.json文件，这是一个标准的npm说明文件","link":"/2020/04/29/webpack/"},{"title":"vue中的数据双向绑定","text":"前言什么是双向数据绑定？当数据发生变化时，视图也会随之发生变化，当视图发生变化时，数据也会随着视图同步变化。数据双向绑定时对于UI控件来说的，非UI控件不会涉及到数据双向绑定。 全局性数据流使用单向，方便跟踪；局部性数据流使用双向，简单易操作。 实现数据双向绑定Vue提供了v-model指令实现数据双向绑定功能 1234567891011121314151617181920212223242526272829303132&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;meta name=\"viewport\" content=\"width=device-width,initial-scale=1.0\"&gt; &lt;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"&gt; &lt;title&gt;Document&lt;/title&gt; &lt;script src=\"./lib/vue-2.4.0.js\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=\"app\"&gt; &lt;h4&gt;{{msg}}&lt;/h4&gt; &lt;!--v-bind只能实现数据的单项绑定，从M自动绑定到V，无法实现数据的双向绑定--&gt; &lt;!-- &lt;input type=\"text\" v-bind:value=\"msg\" style=\"width:100%\"&gt;--&gt; &lt;input type=\"text\" style=\"width:100%\" v-model=\"msg\"&gt;&lt;/div&gt;&lt;script&gt; //创建Vue实例，得到ViewModel var vm=new Vue({ el: '#app', data: { msg: 'helloworld' }, methods: { } });&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 需要注意的是，v-model只能应用在表单元素中","link":"/2020/04/06/vue-databinding/"},{"title":"vue-resource与axios","text":"Vue.js是数据驱动的，不推荐直接操作DOM。因此Vue要实现异步加载可以使用vue-resource或axios，而不需要使用jQuery的DOM选择器。Vue.js2.0版本及以后推荐axios来完成ajax请求。 vue-resourcevue-resource是Vue.js的一款插件，它可以通过XMLHttpRequest或JSONP发起请求并处理响应。也就是说，$.ajax能做的事情，vue-resource插件一样也能做到，而且vue-resource的API更为简洁。另外，vue-resource还提供了非常有用的inteceptor功能，使用inteceptor可以在请求前和请求后附加一些行为，比如使用inteceptor在ajax请求时显示loading界面。 vue-resource特点vue-resource插件具有以下特点： 体积小vue-resource非常小巧，在压缩以后只有大约12KB，服务端启用gzip压缩后只有4.5KB大小，这远比jQuery的体积要小得多。 支持主流的浏览器和Vue.js一样，vue-resource除了不支持IE 9以下的浏览器，其他主流的浏览器都支持。 支持Promise API和URI TemplatesPromise是ES6的特性，Promise的中文含义为“先知”，Promise对象用于异步计算。 URI Templates表示URI模板，有些类似于ASP.NET MVC的路由模板。 支持拦截器拦截器是全局的，拦截器可以在请求发送前和发送请求后做一些处理。 拦截器在一些场景下会非常有用，比如请求发送前在headers中设置access_token，或者在请求失败时，提供共通的处理方式。 vue-resource使用1、引入vue-resource 1&lt;script src=&quot;js/vue.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;js/vue-resource.js&quot;&gt;&lt;/script&gt; 2、引入vue-resource后，可以基于全局的Vue对象使用http，也可以基于某个Vue实例使用http。 123456// 基于全局Vue对象使用http Vue.http.get('/someUrl', [options]).then(successCallback, errorCallback); Vue.http.post('/someUrl', [body], [options]).then(successCallback, errorCallback); // 在一个Vue实例内使用http this.$http.get('/someUrl', [options]).then(successCallback, errorCallback); this.$http.post('/someUrl', [body], [options]).then(successCallback, errorCallback); 在发送请求后，使用then方法来处理响应结果，then方法有两个参数，第一个参数是响应成功时的回调函数，第二个参数是响应失败时得回调函数。 3、then方法的回调函数也有两种写法，第一种是传统的函数写法，第二种是更为简洁的ES 6的Lambda写法： 123456789101112// 传统写法 this.$http.get('/someUrl', [options]).then(function(response){ // 响应成功回调 }, function(response){ // 响应错误回调 }); // Lambda写法 this.$http.get('/someUrl', [options]).then((response) =&gt; { // 响应成功回调 }, (response) =&gt; { // 响应错误回调 }); 4、支持的HTTP方法 vue-resource的请求API是按照REST风格设计的，它提供了7种请求API： 1234567get(url, [options])head(url, [options])delete(url, [options])jsonp(url, [options])post(url, [body], [options])put(url, [body], [options])patch(url, [body], [options]) 除了jsonp以外，另外6种的API名称是标准的HTTP方法。当服务端使用REST API时，客户端的编码风格和服务端的编码风格近乎一致，这可以减少前端和后端开发人员的沟通成本。 5、emulateHTTP的作用 如果Web服务器无法处理PUT, PATCH和DELETE这种REST风格的请求，你可以启用enulateHTTP现象。启用该选项后，请求会以普通的POST方法发出，并且HTTP头信息的X-HTTP-Method-Override属性会设置为实际的HTTP方法。 1Vue.http.options.emulateHTTP = true; 6、emulateJSON的作用 如果Web服务器无法处理编码为application/json的请求，你可以启用emulateJSON选项。启用该选项后，请求会以application/x-www-form-urlencoded作为MIME type，就像普通的HTML表单一样。 Vue.http.options.emulateJSON = true; vue-resource是一个非常轻量的用于处理HTTP请求的插件，它提供了两种方式来处理HTTP请求： 使用Vue.http或this.$http 使用Vue.resource或this.$resource axios vue2.0之后，就不再对vue-resource更新，而是推荐使用axios。基于Promise的HTTP请求客户端，可同时在浏览器和Node.js中使用. 功能特性 在浏览器发送 XMLHttpRequests 请求 在 node.js 中发送 http请求 支持 Promise API 拦截请求和响应 转换请求和响应数据 取消请求 自动转换 JSON 数据 客户端支持保护安全免受 CSRF/XSRF 攻击 axios的使用 安装 axios1$ npm install axios 在要使用的文件中引入axios1import axios from 'axios' GET请求123456789101112131415161718192021// 向具有指定ID的用户发出请求axios.get('/user?ID=12345').then(function (response) {console.log(response);}).catch(function (error) {console.log(error);}); // 也可以通过 params 对象传递参数axios.get('/user', {params: {ID: 12345}}).then(function (response) {console.log(response);}).catch(function (error) {console.log(error);}); POST请求12345678910axios.post('/user', {firstName: 'Fred',lastName: 'Flintstone'}).then(function (response) {console.log(response);}).catch(function (error) {console.log(error);}); 执行多个并发请求123456789101112function getUserAccount() {return axios.get('/user/12345');} function getUserPermissions() {return axios.get('/user/12345/permissions');} axios.all([getUserAccount(), getUserPermissions()]).then(axios.spread(function (acct, perms) {//两个请求现已完成})); axios API：可以通过将相关配置传递给 axios 来进行请求。12345678910111213axios(config)// 发送一个 POST 请求axios({method: 'post',url: '/user/12345',data: {firstName: 'Fred',lastName: 'Flintstone'}});axios(url[, config])// 发送一个 GET 请求 (GET请求是默认请求模式)axios('/user/12345'); 请求方法别名： 为了方便起见，已经为所有支持的请求方法提供了别名。 1234567axios.request（config）axios.get（url [，config]）axios.delete（url [，config]）axios.head（url [，config]）axios.post（url [，data [，config]]）axios.put（url [，data [，config]]）axios.patch（url [，data [，config]]） 注意:当使用别名方法时，不需要在config中指定url，method和data属性。并发帮助函数处理并发请求。axios.all（iterable）axios.spread（callback） 创建实例也可以使用自定义配置创建axios的新实例。axios.create（[config]） 12345var instance = axios.create({baseURL: 'https://some-domain.com/api/',timeout: 1000,headers: {'X-Custom-Header': 'foobar'}}); 实例方法可用的实例方法如下所示。 指定的配置将与实例配置合并。 1234567axios＃request（config）axios＃get（url [，config]）axios＃delete（url [，config]）axios＃head（url [，config]）axios＃post（url [，data [，config]]）axios＃put（url [，data [，config]]）axios＃patch（url [，data [，config]]） 请求配置这些是用于发出请求的可用配置选项。 只有url是必需的。 如果未指定方法，请求将默认为GET。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131{// `url`是将用于请求的服务器URLurl: '/user', // `method`是发出请求时使用的请求方法method: 'get', // 默认 // `baseURL`将被添加到`url`前面，除非`url`是绝对的。// 可以方便地为 axios 的实例设置`baseURL`，以便将相对 URL 传递给该实例的方法。baseURL: 'https://some-domain.com/api/', // `transformRequest`允许在请求数据发送到服务器之前对其进行更改// 这只适用于请求方法'PUT'，'POST'和'PATCH'// 数组中的最后一个函数必须返回一个字符串，一个 ArrayBuffer或一个 Stream transformRequest: [function (data) {// 做任何你想要的数据转换 return data;}], // `transformResponse`允许在 then / catch之前对响应数据进行更改transformResponse: [function (data) {// Do whatever you want to transform the data return data;}], // `headers`是要发送的自定义 headersheaders: {'X-Requested-With': 'XMLHttpRequest'}, // `params`是要与请求一起发送的URL参数// 必须是纯对象或URLSearchParams对象params: {ID: 12345}, // `paramsSerializer`是一个可选的函数，负责序列化`params`// (e.g. https://www.npmjs.com/package/qs, http://api.jquery.com/jquery.param/)paramsSerializer: function(params) {return Qs.stringify(params, {arrayFormat: 'brackets'})}, // `data`是要作为请求主体发送的数据// 仅适用于请求方法“PUT”，“POST”和“PATCH”// 当没有设置`transformRequest`时，必须是以下类型之一：// - string, plain object, ArrayBuffer, ArrayBufferView, URLSearchParams// - Browser only: FormData, File, Blob// - Node only: Streamdata: {firstName: 'Fred'}, // `timeout`指定请求超时之前的毫秒数。// 如果请求的时间超过'timeout'，请求将被中止。timeout: 1000, // `withCredentials`指示是否跨站点访问控制请求// should be made using credentialswithCredentials: false, // default // `adapter'允许自定义处理请求，这使得测试更容易。// 返回一个promise并提供一个有效的响应（参见[response docs]（＃response-api））adapter: function (config) {/* ... */}, // `auth'表示应该使用 HTTP 基本认证，并提供凭据。// 这将设置一个`Authorization'头，覆盖任何现有的`Authorization'自定义头，使用`headers`设置。auth: {username: 'janedoe',password: 's00pers3cret'}, // “responseType”表示服务器将响应的数据类型// 包括 'arraybuffer', 'blob', 'document', 'json', 'text', 'stream'responseType: 'json', // default //`xsrfCookieName`是要用作 xsrf 令牌的值的cookie的名称xsrfCookieName: 'XSRF-TOKEN', // default // `xsrfHeaderName`是携带xsrf令牌值的http头的名称xsrfHeaderName: 'X-XSRF-TOKEN', // default // `onUploadProgress`允许处理上传的进度事件onUploadProgress: function (progressEvent) {// 使用本地 progress 事件做任何你想要做的}, // `onDownloadProgress`允许处理下载的进度事件onDownloadProgress: function (progressEvent) {// Do whatever you want with the native progress event}, // `maxContentLength`定义允许的http响应内容的最大大小maxContentLength: 2000, // `validateStatus`定义是否解析或拒绝给定的promise// HTTP响应状态码。如果`validateStatus`返回`true`（或被设置为`null` promise将被解析;否则，promise将被 // 拒绝。validateStatus: function (status) {return status &gt;= 200 &amp;&amp; status &lt; 300; // default}, // `maxRedirects`定义在node.js中要遵循的重定向的最大数量。// 如果设置为0，则不会遵循重定向。maxRedirects: 5, // 默认 // `httpAgent`和`httpsAgent`用于定义在node.js中分别执行http和https请求时使用的自定义代理。// 允许配置类似`keepAlive`的选项，// 默认情况下不启用。httpAgent: new http.Agent({ keepAlive: true }),httpsAgent: new https.Agent({ keepAlive: true }), // 'proxy'定义代理服务器的主机名和端口// `auth`表示HTTP Basic auth应该用于连接到代理，并提供credentials。// 这将设置一个`Proxy-Authorization` header，覆盖任何使用`headers`设置的现有的`Proxy-Authorization` 自定义 headers。proxy: {host: '127.0.0.1',port: 9000,auth: : {username: 'mikeymike',password: 'rapunz3l'}}, // “cancelToken”指定可用于取消请求的取消令牌// (see Cancellation section below for details)cancelToken: new CancelToken(function (cancel) {})} 使用 then 时，将收到如下响应： 12345678axios.get('/user/12345').then(function(response) {console.log(response.data);console.log(response.status);console.log(response.statusText);console.log(response.headers);console.log(response.config);}); 配置默认值1、全局axios默认值 123axios.defaults.baseURL = 'https://api.example.com';axios.defaults.headers.common['Authorization'] = AUTH_TOKEN;axios.defaults.headers.post['Content-Type'] = 'application/x-www-form-urlencoded'; 2、自定义实例默认值 123456//在创建实例时设置配置默认值var instance = axios.create（{ baseURL：'https://api.example.com'}）;//在实例创建后改变默认值instance.defaults.headers.common ['Authorization'] = AUTH_TOKEN; 3、配置优先级顺序配置将与优先顺序合并。 顺序是lib / defaults.js中的库默认值，然后是实例的defaults属性，最后是请求的config参数。 后者将优先于前者。 这里有一个例子。 123456789101112//使用库提供的配置默认值创建实例//此时，超时配置值为`0`，这是库的默认值var instance = axios.create（）; //覆盖库的超时默认值//现在所有请求将在超时前等待2.5秒instance.defaults.timeout = 2500; //覆盖此请求的超时，因为它知道需要很长时间instance.get('/ longRequest'，{ timeout：5000}); 拦截器你可以截取请求或响应在被 then 或者 catch 处理之前 1234567891011121314151617//添加请求拦截器axios.interceptors.request.use(function(config){ //在发送请求之前做某事 return config; }，function（error）{ //请求错误时做些事 return Promise.reject(error); }); //添加响应拦截器axios.interceptors.response.use(function(response){ //对响应数据做些事 return response; }，function(error){ //请求错误时做些事 return Promise.reject（error）; }); 如果你以后可能需要删除拦截器。 12var myInterceptor = axios.interceptors.request.use(function () {/*...*/});axios.interceptors.request.eject(myInterceptor); 你可以将拦截器添加到axios的自定义实例。 12var instance = axios.create();instance.interceptors.request.use(function () {/*...*/}); 处理错误 1234567891011121314axios.get('/ user / 12345') .catch(function(error){ if(error.response){ //请求已发出，但服务器使用状态代码进行响应 //落在2xx的范围之外 console.log（error.response.data）; console.log（error.response.status）; console.log（error.response.headers）; } else { //在设置触发错误的请求时发生了错误 console.log（'Error'，error.message）; }} console.log（error.config）; }); 您可以使用validateStatus配置选项定义自定义HTTP状态码错误范围。 12345axios.get('/ user / 12345'，{ validateStatus：function（status）{ return status &lt; 500; //仅当状态代码大于或等于500时拒绝 }}}) 消除您可以使用取消令牌取消请求。axios cancel token API基于可取消的promise提议，目前处于阶段1您可以使用CancelToken.source工厂创建一个取消令牌，如下所示： 12345678910111213141516var CancelToken = axios.CancelToken;var source = CancelToken.source（）; axios.get('/user/12345', {cancelToken: source.token}).catch(function(thrown) {if (axios.isCancel(thrown)) {console.log('Request canceled', thrown.message);} else {// 处理错误}}); //取消请求（消息参数是可选的）source.cancel（'操作被用户取消。'）; 您还可以通过将执行器函数传递给CancelToken构造函数来创建取消令牌： 123456789101112var CancelToken = axios.CancelToken;var cancel; axios.get（'/ user / 12345'，{ cancelToken：new CancelToken（function executor（c）{ //一个执行器函数接收一个取消函数作为参数 cancel = c; }）}）; // 取消请求clear(); 注意：您可以使用相同的取消令牌取消几个请求。 使用application / x-www-form-urlencoded格式默认情况下，axios将JavaScript对象序列化为JSON。 要以应用程序/ x-www-form-urlencoded格式发送数据，您可以使用以下选项之一。 1、浏览器在浏览器中，您可以使用URLSearchParams API，如下所示： 1234var params = new URLSearchParams();params.append('param1', 'value1');params.append('param2', 'value2');axios.post('/foo', params); 请注意，所有浏览器都不支持URLSearchParams，但是有一个polyfill可用（确保polyfill全局环境）。或者，您可以使用qs库对数据进行编码： 12var qs = require('qs');axios.post('/foo', qs.stringify({ 'bar': 123 }); 2、Node.js在node.js中，可以使用querystring模块，如下所示： 12var querystring = require('querystring');axios.post('http://something.com/', querystring.stringify({ foo: 'bar' }); 3、TypeScriptaxios包括TypeScript定义。 12import axios from 'axios';axios.get('/user?ID=12345'); axios在很大程度上受到Angular提供的$http服务的启发。 最终，axios努力提供一个在Angular外使用的独立的$http-like服务。 vue-resource和axios的区别vue-resources不再更新了，vue作者尤大推荐axios。 axios在浏览器里建立XHR通过nodejs进行http请求转换或者拦截请求数据或响应数据支持Promise的API可以取消请求自动转换JSON可以防御XSRF攻击！ vue-resources只提供了浏览器版本","link":"/2020/04/14/vue-resource/"},{"title":"MySQL45讲","text":"MySQL45讲一、MySQL基础架构MySQL基本架构示意图： 客户端--》连接器--》分析器（语法分析，词法分析，查询缓存若命中则直接返回结果）--》优化器(执行计划生成，索引选择）--》执行器（操作引擎，返回结果）--》存储引擎层（存储数据，提供读写接口） 大体来说可以分为Server层（涵盖MySQL大部分核心服务功能）和存储引擎层（负责数据的存储和提取）两部分。 常见的存储引擎有InnoDB，MyISAM，Memory，现在最常用的存储引擎是InnoDB，它从MySQL5.5.5开始成为了默认存储引擎。 连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接命令一般是这样： 1mysql -h$ip -P$port -u$user -p 连接命令中的mysql是客户端工具，用来跟服务端建立连接，在完成了经典的TCP握手后，连接器就要开始认证你的身份。 如果用户名密码认证通过，连接器会到权限表中查出你拥有的权限，之后这个连接里面的权限判断逻辑都依赖于此时读到的权限，建立连接成功后无法修改权限。 客户端若长时间未使用则连接器将其自动断开，自动断开的时间由参数wait_timeout控制，默认8小时。 建立连接：尽量使用长连接（减少建立连接的动作） 过多的长连接使内存占用过大，被系统强行杀掉（OOM），从现象看就是MySQL异常重启了，如何解决这个问题？ 定期断开长连接 如果使用的使MySQL5.7或以上版本，可在每次执行一个比较大的操作后，通过执行mysql_reset_connection来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。 查询缓存（MySQL8.0之后没有这个功能了）连接建立完成后，执行逻辑就会来到第二步：查询缓存。 如果语句在查询缓存中，则将结果直接返回给客户端。 如果语句不在查询缓存中，则继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。 但是大多数情况下不建议使用查询缓存，为什么呢？因为查询缓存往往弊大于利。 查询缓存的失效非常频繁，只要有对一个表的更新，这个表上的所有查询缓存都会被清空。除非你的业务是有一张静态表，很长时间才会更新一次。 将参数query_cache_type设置成DEMAND，这样对于默认的SQL语句都不使用查询缓存。对于要使用查询缓存的语句，可以用SQL_CACHE显式指定。 分析器（分析语句是否存在错误）分析器负责词法分析、语法分析 优化器（决定使用哪一种执行方案）当表里面有多个索引时，决定使用哪个索引；或者当一个语句中有多表关联（join）时，决定各个表的连接顺序。优化器的作用就是决定选择使用哪一种方案。 执行器（判断有无权限再执行）执行器负责执行语句，开始执行时会先判断一下你对这个表有没有执行查询的权限，如果没有，报错；如果有，就继续执行，执行器会根据表的引擎定义，去使用这个引擎提供的接口。 在数据库的慢查询日志中会看到一个rows_examined的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行时累加的。 在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟rows_examined并不是完全相同的。 二、日志系统与查询流程不一样的是，更新流程还涉及两个重要的日志模块，redo log（重做日志）和binlog（归档日志）。 重要的日志模块：redo logMySQL中的WAL技术，WAL的全称是Write-Ahead Logging，它的关键点是先写日志，再写磁盘。 具体来说，当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log里面，并更新内存，这个时候更新就算完成了。同时，InnoDB引擎会在适当（比较空闲）的时候，将这个操作记录更新到磁盘里面。 InnoDB的redo log是固定大小的 write pos是当前记录的位置，一边写一边后移。checkpoint是当前要擦除的位置，也是往后推移并且循环的。 有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。 重要的日志模块：binlogredo log是引擎特有的日志，而Server层的日志为binlog（归档日志）。 redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑。 redo log的写入分为两个步骤：prepare和commit，这就是“两阶段提交”。redo log和binlog都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。 小结redo log用于保证crash-safe能力。innodb_flush_log_at_trx_commit这个参数设置为1的时候，表示每次事务的redo log都直接持久化到磁盘。这个参数建议设置为1，这样可保证MySQL异常重启后数据不丢失。 sync_binlog这个参数设置为1时，表示每次事务的binlog都持久化到磁盘。这个参数也建议设置为1，这样可以保证MySQL异常重启后binlog不丢失。 两阶段提交是跨系统维持数据逻辑一致性时常用的一个方案。 Binlog有两种模式，statement格式的话是记sql语句，row格式会记录行的内容，记两条，更新前和更新后都有。 三、事务隔离简单来说，事务就是保证一组数据库操作，要么全部成功，要么全部失败。在MySQL中，事务支持是在引擎层实现的。你现在知道，MySQL是一个支持多引擎的系统，但并不是所有的引擎都支持事务。比如MySQL的原生的MyISAM引擎就不支持事务。 隔离性和隔离级别ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性） 若数据库上有多个事务同时执行时，就可能出现脏读、不可重复读、幻读的问题，为了解决这些问题，就有了“隔离级别”的概念。 SQL标准的事务隔离级别包括：读未提交、读提交（默认）、可重复读和串行化。可重复读：事务在执行期间看到的数据前后必须是一致的。 事务隔离的实现在MySQL中，实际上每条记录在更新的时候都会同时记录一条回滚操作（当无事务需用到这些回滚日志时，回滚日志会被删除），都可以得到前一个状态的值。 假设一个值从1被按顺序改成了2、3、4，当前值是4，但是查询这条记录时，不同时刻启动的事务会有不同的read-view。如该例中，在视图A、B、C里面，这一个记录的值分别是1、2、4，同一条记录在系统中可以有多个版本，MVCC（多版本并发控制）。 ​ 尽量不要使用长事务。 长事务意味着系统里面会存在很老的事务视图。 在MySQL5.5及以前的版本，回滚日志是跟数据字典一起放在ibdata文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。 除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库。 事务的启动方式MySQL事务启动方式有以下几种： 显式启动事务语句，begin或start transaction。配套的提交语句是commit，回滚语句rollback。 set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个select语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在知道你主动执行commit或rollback语句，或者断开连接。（忘了提交很可能造成长事务） 建议总是使用set autocommit=1，通过显式语句来启动事务。 四、索引（上）索引的出现是为了提高数据查询的效率，就像书的目录一样。 索引的常见模型实现索引的方式有很多种，可以用于提高读写效率的数据结构有很多，如以下三种常见的数据结构：哈希表、有序数组和搜索树。 哈希表是一种以键-值（key-value)存储数据的结构，我们只要输入待查找的值即key，用一个哈希函数把key换算成一个确定的位置，就可以找到其对应的值即Value。 不可避免地，多个key值经过哈希函数的换算，会出现同一个值的情况。处理这种请况的方法是，拉出一个链表。 因为不是有序的，所以哈希索引做区间查询的速度是很慢的。 哈希表这种结构适用于只有等值查询的场景，比如Memcached及其他一些NoSQL引擎。 有序数组（查询效率高，更新数据成本高）在等值查询和范围查询场景中的性能都非常优秀。单看查询效率，有序数组就是最好的数据结构了，但是，在需要更新数据时就很麻烦了，往中间插入一个记录就必须挪动后面所有的记录，成本太高。 因此，有序数组索引只适用于静态存储引擎（即不会再修改的数据） 搜索树 二叉树是搜索效率最高的，但实际上大多数数据库存储并不使用二叉树，原因是索引不止存在内存中，还要写到磁盘上。为了尽量少地读磁盘，必须让查询过程访问尽量少的数据块，因此要使用“N叉”树，这里的“N”取决于数据块的大小。 在MySQL中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索引的工作方式并不一样。而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同。 InnoDB的索引模型在InnoDB中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。InnoDB使用了B+树索引模型，所以数据都是存储在B+树中的。 每一个索引在InnoDB里面对应一棵B+树。 假设，我们有一个主键列为ID的表，表中有字段k，并且在k上有索引。 这个表的建表语句是： 12345mysql&gt; create table T(id int primary key,k int not null,name varchar(16),index (k))engine=InnoDB; 索引类型分为主键索引和非主键索引，主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引 ；非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引 。 基于主键索引和普通索引的查询有什么区别？ 主键索引查询，只需搜索主键这棵B+树；普通索引查询，需先搜索该索引树，得到对应的主键的值，再到主键索引树搜索一次，这个过程称为回表。 在应用中应该尽量使用主键查询。 五、索引（下）索引维护B+树为了维护索引有序性，在插入新值时需要做必要的维护。当需要在某个记录后面插入一个新记录时，该记录所在的数据页已经满了，根据B+树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂。在这种情况下，性能自然会受影响，还影响数据页的利用率。 当相邻两个页由于删除了数据，利用率很低时，会将数据页做合并。合并的过程，可以认为是分裂过程的逆过程。 自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的： 1NOT NULL PRIMARY KEY AUTO_INCREMENT 主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。因此，自增主键往往是更合理的选择。 有些业务的场景需求是这样的：1.只有一个索引；2.该索引必须是唯一索引。这就是典型的KV场景。这时候就要优先考虑“尽量使用主键查询”原则，将索引设置为主键。 覆盖索引由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。 覆盖索引是指，索引上的信息足够满足查询请求，不需要再回到主键索引上去取数据。 。。。 最左前缀原则B+树这种索引结构，可以利用索引的“最左前缀”，来定位记录。 。。。 索引下推MySQL5.6引入的索引下推优化，可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。 六、全局锁和表锁根据加锁的范围，MySQL里面的锁大致可以分为全局锁、表级锁和行锁三类。 全局锁全局锁就是对整个数据库实例加锁。MySQL提供了一个加全局读锁的方法，命令是Flush tables with read lock（FTWRL）。 全局锁的典型使用场景是，做全局逻辑备份。 不加锁的话，备份系统备份得到的库不是一个逻辑时间点，这个视图是逻辑不一致的。 前面讲事务隔离时，其实是有一个方法能够拿到一致性视图的：在可重复读隔离级别下开启一个事务。 官方自带的逻辑备份工具是mysqldump，当mysqldump使用参数-single-transaction时，导数据前就会启动一个事务，来确保拿到一致性视图。而由于MVCC的支持，这个过程中数据时可以正常更新的。 single-transaction方法只适用于所有的表使用事务引擎的库。有的表使用了不支持事务的引擎，那么备份就只能通过FTWRL方法。 表级锁MySQL里面的表级锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL）。 表锁的语法是lock tables … read/write。与FTWRL类似，可以用unlock tables主动释放锁，也可以在客户端断开时自动释放。 lock tables语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。 在还没有出现更细粒度的锁的时候，表锁是最常用的处理并发的方式。而对于InnoDB这种支持行锁的引擎，一般不使用lock tables命令来控制并发，毕竟锁住整个表的影响面还是太大。 另一类表级锁是MDL（metadata lock，MySQL5.5开始出现）。MDL不需要显式使用，在访问一个表时会被自动加上。MDL的作用是，保证读写的正确性。 读锁之间不互斥，因此可以有多个线程同时对一个表增删改查。 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的 安全性。 事务中的MDL锁，在语句执行开始时申请，但语句结束后不会马上释放，而等到整个事务提交后再释放。 在MySQL的information_schema库的innodb_trx表中，可以查到当前执行中的事务。 七、行锁：怎么减少行锁对性能的影响MySQL的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁。行锁是针对数据表中行记录的锁。 从两阶段锁说起在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就马上释放，而是要等到事务结束了才释放锁，这个就是两阶段锁协议。因此，如果当前事务需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。 死锁和死锁检测当出现死锁之后，有两种策略： 直接进入等待，直到超时，这个超时时间可以通过参数innodb_lock_wait_timeout（默认值为50s）来设置。 发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect设置为on,表示开启这个逻辑。 通常情况下采用第二种策略，主动死锁检测，而且innodb_deadlock_detect的默认值本身就是on。但死锁检测要耗费大量的CPU资源。 如何减少死锁经检测导致的性能问题？ 若能确保该业务一定不会出现死锁，可以临时把死锁检测关掉。有风险 控制并发度，在数据库服务端做并发控制。如果有中间件，可以考虑在中间件实现；如果团队中有能修改MySQL源码的人，也可以做在MySQL里面。 也可以将一行改成逻辑上的多行来减少锁冲突的机率。 八、事务到底是隔离的还是不隔离的？begin/start transaction命令并不是一个事务的起点，在执行到它们之后的第一个操作InnoDB表的语句，事务才真正启动。如果你想要马上启动一个事务，可以使用start transaction with consistent snapshot这个命令。 第一种启动方式，一致性视图是在第一个快照读语句时创建的； 第二种启动方式，一致性视图是在执行start transaction with consistent snapshot时创建的。 在MySQL里，有两个”视图“的概念： ​ 一个是view。他是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法create view…，而它的查询方法与表一样。 ​ 另一个是InnoDB在实现MVCC时用到的一致性读视图，即consistent read view，用于支持RC（Read Committed，读提交）和RR（Repeatable Read,可重复读）隔离级别的实现。 ”快照“在MVCC里是怎么工作的？在可重复读隔离级别下，事务在启动时就拍了个“快照”。这个快照是基于整库的。 InnoDB里面每个事务有一个唯一的事务ID，叫做transaction id。它是在事务开始时向InnoDB的事务系统申请的，是按照申请顺序严格递增的。 InnoDB利用了”所有数据都有多个版本“的这个特性，实现了”秒级创建快照“的能力。 更新逻辑更新数据都是先读后写的，而这个读，只能读当前的值，称为”当前读“。除了update语句外，select语句如果加锁，也是当前读。 当前读，总是读取已经提交完成的最新版本。 九、普通索引和唯一索引从普通索引和唯一索引对查询语句和更新语句的性能影响来进行分析。 查询过程对于普通索引来说，查找到满足条件的第一个记录后，需要查找下一个记录，直到碰到第一个不满足k=5条件的记录。 对于唯一索引来说，由于索引定义唯一性，查找到第一个满足条件的记录后，就会停止继续检索。 InnoDB的数据是按数据页为单位来读写的。也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在InnoDB种，每个数据页的大小默认是16KB。 更新过程当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InnoDB会将这些更新操作缓存在change buffer（它是可以持久化的数据，在内存中有拷贝，也会被写入到磁盘中），这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的 时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方 式就能保证这个数据逻辑的正确性。 如果某次写入使用了change buffer机制，之后主机异常重启，不会丢失change buffer和数据。 将change buffer中的操作应用到原数据页，得到最新结果的过程称为merge。除了访问这个数据页会触发merge外，系统有后台线程会定期merge。在数据库正常关闭（shutdown）的过程中，也会执行merge操作。 merge的执行流程： 从磁盘读入数据页到内存（老版本的数据页）； 从change buffer里找出这个数据页的change buffer记录（可能有多个），依次应用，得到新版数据页； 写redo log。这个redo log包含了数据的变更和change buffer的变更。 merge过程并不会把数据直接写回磁盘。 什么条件下可以使用change buffer呢？ 对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。比如，要插入（4，400）这个记录，就要先判断现在表中是否已经存在k=4的记录，而这必须要将数据页读入内存才能判断。如果都已经读入到内存了，那直接更新内存会更快，就没必要使用change buffer了。 因此，唯一索引的更新就不能使用change buffer，实际上也只有普通索引可以使用。 change buffer用的是buffer pool里的内存，因此不能无限增大，change buffer的大小，可以通过参数innodb_change_buffer_max_size来动态设置。这个参数设置为50时，表示change buffer的大小最多只能占用buffer pool的50%。 对于写多读少的业务来说，change buffer的使用效果最好。 索引选择和实践普通索引和唯一索引应该怎么选择。其实，这两类索引在查询能力上是没差别的，主要考虑的是对更新性能的影响。建议尽量选择普通索引。 如果所有的更新后面，都马上伴随着对这个记录的查询，那么应该关闭change buffer。而在其他情况下，change buffer都能提升更新性能。 change buffer和redo log写： 该记录在内存中，直接更新内存；并将该动作记入redo log。 该记录不在内存，就在内存中的change buffer区域记录下要更新该记录的信息。并将在change buffer区域记录相关信息这个动作记入redo log。 读： 直接从内存返回。 将该记录从磁盘读入内存中，然后应用change buffer里面的操作日志，生成一个正确的版本并返回结果。 redo log主要节省的是随机写磁盘的IO消耗（转成顺序写），而change buffer主要节省的是随机读磁盘的IO消耗。 十、MySQL为什么有时候会选错索引？MySQL中一张表其实是可以支持多个索引的。但写SQL语句时，并没有主动指定使用哪个索引，所以有时候MySQL会选错了索引，导致执行速度变得很慢。 优化器的逻辑在第一章中有讲到，选择索引是优化器的工作。 优化器选择索引的目的，是找到一个最优的执行方案，并用最小的代价去执行语句。在数据库里面，扫描行数是影响执行代价的因素之一。扫描行数并不是唯一的判断标准，优化器还会结合是否使用临时表、是否排序等因素进行综合判断。 MySQL在判断扫描行数是为何选错索引，扫描行数是怎么判断的？ ​ MySQL 在真正开始执行语句之前，并不能精确地知道 满足这个条件的记录有多少条，而只能根据统计信息来 估算记录数。​ 这个统计信息就是索引的“区分度”。显然，一个索引 上不同的值越多，这个索引的区分 度就越好。而一个索 引上不同的值的个数，我们称之为“基数”(cardinality)。 也就是说，这个基数越大，索引的区分度越好。 MySQL是怎样得到索引的基数的呢？ 采样统计，InnoDB默认会选择N个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。 当统计信息不对时，可以使用analyze table t命令，来重新统计索引信息。 而数据表是会持续更新的，索引统计信息也不会固定不变。所以，当变更的数据行数超过1/M的时候，会自动触发重新做一次索引统计。 在MySQL中，有两种存储索引统计的方式,可以通过设置参数innodb_stats_persistent的值来选择： 设置为on的时候，表示统计信息会持久化存储。这是、时，默认的N是20，M是10。 设置为off的时候，表示统计信息只存在内存中，这时，默认的N是8，M是16。 使用普通索引需要把回表的代价算进去。 索引选择异常和处理其实大多数时候优化器都能找到正确的索引，但偶尔还是会选择错误，怎么办： 一种方法是，采用force index强行选择一个索引。使用force index最主要的问题还是变更的及时性。开发时通常不会先写上force index，而是等到线上出现问题的时候，才会去修改SQL语句，加上force index，但对于生产系统来说，这个过程不够敏捷。 所以，数据库的问题最好还是在数据库内部来解决。 第二种方法是，我们可以考虑修改语句，引导MySQL使用我们期望的索引。比如，把“order by b limit 1” 改成 “order by b,a limit 1” ，语义的逻辑是相同的。 改之前优化器选择使用索引 b，是因为它认为使用索引 b 可以避免排序（b 本身是索引，已经是有序的了，如果选择索引 b 的话，不需要再做排序，只需要遍历），所以即使扫描行数多，也判定为代价更小。现在 order by b,a 这种写法，要求按照 b,a 排序，就意味着使用这两个索引都需要排序。 因此，扫描行数成了影响决策的主要条件，于是此时优化器选了只需要扫描 1000 行的索引 a。这个方法不具备通用性。 第三种方法是，在有些场景下，我们可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引。 十一、给字符串字段加索引MySQL是支持前缀索引的，也就是说，可以定义字符串的一部分作为索引。默认地，如果创建索引的语句不指定前缀长度，那么索引就会包含整个字符串。前缀索引的优势，占用空间会更小。前缀索引的劣势，可能会导致查询语句读数据的次数变多。 使用前缀索引很可能会损失区分度，所以你需要预先设定一个可以接受的损失比例，比如5%。 前缀索引对覆盖索引的影响使用前缀索引就用不上覆盖索引对查询性能的优化了，这也是你在选择是否使用前缀索引时需要考虑的一个因素。 其他方式假设维护的数据库是一个市的公民信息系统，这时候如果对身份证号做长度为6的前缀索引的话，这个索引的区分度就非常低了。可能要创建长度为12以上的前缀索引才能满足区分度的要求。但是，索引选取的越长，占用的磁盘空间就越大，相同的数据页能放下的索引值就越少，搜索效率就越低。 如果我们能够确定业务需求里面只有按照身份证进行等值查询的需求，可以使用其他的处理方式，既可以占用更小的空间，也能达到相同的查询效率。 第一种方式是使用倒序存储。如果你存储身份证号的时候把它倒过来存，即使取6位也可以提供足够的区分度。 第二种方式是使用hash字段。可以在表中再创建一个整数字段，来保存身份证的校验码，同时在这个字段上创建索引。 使用倒序存储和使用hash字段这两种方法的异同点： 相同点是，都不支持范围查询，倒序存储的字段上创建的索引是按照倒序字符串的方式排序的，已经没有办法利用索引方式查出身份证号码在[ID_X,ID_Y]的所有市民了。同样的，hash字段的方式也只能支持等值查询。 区别主要体现在以下三个方面： 1.从占用的额外空间来看，倒序存储方式在主键索引上，不会消耗额外的存储空间，而hash字段方法需要增加一个字段，当然，倒序存储方式使用4个字节的前缀长度应该是不够的，如果再长一点，这个消耗跟额外这个hash字段也差不多抵消了。 2.在CPU消耗方面，倒序方式每次写和读的时候，都需要额外调用一次reverse函数，而hash字段的方式需要额外调用一次crc32（）函数。如果只从这两个函数的计算复杂度来看，reverse函数额外消耗的CPU资源会更小些。 3.从查询效率上看，使用hash字段方式的查询性能相对更稳定一些，因为crc32算出来的值虽然有冲突的可能，但概率非常小，可以认为每次查询的平均扫描行数接近1。而倒序存储方式毕竟还是用的前缀索引的方式，也就是说还是会增加扫描行数。 十二、为什么我的MySQL会”抖“一下？一条SQL语句，正常执行的时候特别快，但是有时也不知道怎么回事，它就会变得特别慢，并且这样的场景很难复现，它不只随机，而且持续时间还很短。看上去，这就像是数据库”抖”了一下。今天，我们就一起来看一看这是什么原因。 你的SQL语句为什么变“慢”了当内存数据页跟磁盘数据页内容不一致时，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内存就一致了，称为“干净页”。 平时执行很快的更新操作，其实就是在写内存和日志，而MySQL偶尔“抖”一下的哪个瞬间，可能就是在刷脏页（flush）。 什么情况会引发数据库的flush过程呢？ InnoDB的redo log写满了。（这种情况是InnoDB要尽量避免的） 系统内存不足。当需要新的内存页，而内存不够用时，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到磁盘。（常态） MySQL认为系统“空闲”的时候。 MySQL正常关闭的情况。 刷脏页虽然是常态，但是以下两种情况都是会明显影响性能的： 一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长； 日志写满，更新全部堵住，写性能跌为0，这种情况对敏感业务来说，是不能接受的。 InnoDB刷脏页的控制策略通过innodb_io_capacity参数可以告诉InnoDB本主机的IO能力（磁盘能力）。这个值建议设置为磁盘的IOPS。磁盘的IOPS可以通过fio这个工具来测试。正确地设置innodb_io_capacity参数，InnoDB才能知道需要全力刷脏页时，可以刷多快。 InnooDB的刷盘速度参考两个因素：一个是脏页比例，一个是redo log写盘速度。 参数innodb_max_dirty_pages_pct是脏页比例上限，默认值是75%。InnoDB会根据当前的脏页比例（假设为M），算出一个范围在0到100之间的数字。计算这个数字的伪代码类似这样： 123456F1(M){ if M&gt;=innodb_max_dirty_pages_pct then return 100; return 100*M/innodb_max_dirty_pages_pct;} InnoDB每次写入的日志都有一个序号，当前写入的序号跟 checkpoint 对应的序号之间 的差值，我们假设为 N。InnoDB 会根据这个 N 算出一个范围在 0 到 100 之间的数字， 这个计算公式可以记为 F2(N)。F2(N) 算法比较复杂，你只要知道 N 越大，算出来的值越 大就好了。 然后，根据上述算得的F1(M)和F2(N)两个值，取其中较大的值记为R，之后引擎就可以按照innodb_io_capacity定义的能力乘以R%来控制刷脏页的速度。 合理设置innodb_io_capacity;多关注脏页比例。 一旦一个查询请求需要在执行过程中先 flush 掉一个脏页时，这个查询就可能要比平时慢了。而 MySQL 中的一个机制，可能让你的查询会更慢：在准备刷一个脏页的时候，如果 这个数据页旁边的数据页刚好是脏页，就会把这个“邻居”也带着一起刷掉；而且这个 把“邻居”拖下水的逻辑还可以继续蔓延，也就是对于每个邻居数据页，如果跟它相邻的数据页也还是脏页的话，也会被放到一起刷。 在 MySQL 8.0 中，innodb_flush_neighbors 参数的默认值已经是 0 了。 十三、为什么表数据删除掉一半，表文件大小不变？一个InnoDB表包含两部分，即：表结构定义和数据。在MySQL8.0之前，表结构是存在以.frm为后缀的文件里。而MySQL8.0，则允许把表结构定义放在系统数据表中了。因为表结构定义占用的空间很小，所以我们今天主要讨论的是表数据。 参数innodb_file_per_table表数据既可以存在共享表空间里，也可以是单独的文件。这个行为是由参数innodb_file_per_table控制的： 这个参数设置为OFF表示的是，表的数据放在系统共享表空间，也就是跟数据字典放在一起； 这个参数设置为ON表示的是，每个InnoDB表数据存储在一个以.ibd为后缀的文件中。 从MySQL5.6.6版本开始，它的默认值就是ON了。 表中的数据被删除了，但是表空间却没有被回收。 数据删除流程当我们删掉某条记录时，InnoDB引擎只会把该记录标记为删除。如果在该位置要再插入一个记录，可能会复用这个位置。 InnoDB的数据是按页存储的，那么如果我们删掉了一个数据页上的所有记录，则整个数据就可以被复用了。 但是，数据页的复用跟记录的复用是不同的。 记录的复用，只限于符合范围条件的数据。而当整个页从B+树里面摘掉以后，可以复用到任何位置。 如果相邻的两个数据页利用率都很小，系统就会把这两个页上的数据合到其中一个页上，另外一个数据页就被标记为可复用。 delete命令其实只是把记录的位置，或者数据页标记为了“可复用”，但磁盘文件的大小是不会变的。看起来就像是“空洞”。 实际上，不止是删除数据会造成空洞，插入数据也会。 如果数据是按照索引递增顺序插入的，那么索引是紧凑的。但如果数据是随机插入的，就可能造成索引的数据页分裂。 经过大量增删改的表，都是可能存在空洞的。所以，如果能够把这些空洞去掉，就能达到收缩表空间的目的。 而重建表，就可以达到这样的目的。 重建表表 A，需要做空间收缩，为了把表中存在的空洞去掉，你可以怎么做呢？你可以新建一个与表 A 结构相同的表 B，然后按照主键 ID 递增的顺序，把数据一行一行 地从表 A 里读出来再插入到表 B 中。由于表 B 是新建的表，所以表 A 主键索引上的空洞，在表 B 中就都不存在了。显然地， 表 B 的主键索引更紧凑，数据页的利用率也更高。如果我们把表 B 作为临时表，数据从表 A 导入表 B 的操作完成后，用表 B 替换 A，从效果上看，就起到了收缩表 A 空间的作用。 这里，你可以使用 alter table A engine=InnoDB 命令来重建表。在 MySQL 5.5 版本之 前，这个命令的执行流程跟我们前面描述的差不多，区别只是这个临时表 B 不需要你自己 创建，MySQL 会自动完成转存数据、交换表名、删除旧表的操作。 花时间多的步骤是往临时表插入数据的过程，如果在这个过程中，有新的数据要 写入到表 A 的话，就会造成数据丢失。因此，在整个 DDL 过程中，表 A 中不能有更新。 也就是说，这个 DDL 不是 Online 的。 而在MySQL 5.6 版本开始引入的 Online DDL，对这个操作流程做了优化。我给你简单描述一下引入了 Online DDL 之后，重建表的流程： 建立一个临时文件，扫描表 A 主键的所有数据页； 用数据页中表 A 的记录生成 B+ 树，存储到临时文件中； 生成临时文件的过程中，将所有对 A 的操作记录在一个日志文件（row log）中，对应 的是图中 state2 的状态； 临时文件生成后，将日志文件中的操作应用到临时文件，得到一个逻辑数据上与表 A 相 同的数据文件，对应的就是图中 state3 的状态； 用临时文件替换表 A 的数据文件。 Online和inplace说到 Online，我还要再和你澄清一下它和另一个跟 DDL 有关的、容易混淆的概念 inplace 的区别。你可能注意到了，在图 3 中，我们把表 A 中的数据导出来的存放位置叫作 tmp_table。这 是一个临时表，是在 server 层创建的。在图 4 中，根据表 A 重建出来的数据是放在“tmp_file”里的，这个临时文件是 InnoDB 在内部创建出来的。整个 DDL 过程都在 InnoDB 内部完成。对于 server 层来说，没有把 数据挪动到临时表，是一个“原地”操作，这就是“inplace”名称的来源。所以，我现在问你，如果你有一个 1TB 的表，现在磁盘间是 1.2TB，能不能做一个 inplace 的 DDL 呢？答案是不能。因为，tmp_file 也是要占用临时空间的。 DDL 过程如果是 Online 的，就一定是 inplace 的； 反过来未必，也就是说 inplace 的 DDL，有可能不是 Online 的。截止到 MySQL 8.0，添加全文索引（FULLTEXT index）和空间索引 (SPATIAL index) 就属于这种情况。 使用 optimize table、analyze table 和 alter table 这三种方式重建表的区别。从 MySQL 5.6 版本开始，alter table t engine = InnoDB（也就是 recreate）默认的 就是Online DDL的流程了；analyze table t 其实不是重建表，只是对表的索引信息做重新统计，没有修改数据，这个过程中加了 MDL 读锁；optimize table t 等于 recreate+analyze。 小结在重建表的时候，InnoDB 不 会把整张表占满，每个页留了 1/16 给后续的更新用。也就是说，其实重建表之后不 是“”紧凑的。 现在你已经知道了，如果要收缩一个表，只是delete掉表里面不用的数据的话，表文件的大小是不会变的，你还要通过alter table命令重建表，才能达到表文件变小的目的。重建表的两种实现方式，Online DDL的方式是可以考虑再业务低峰期使用的，而MySQL5.5及以前的版本，这个命令是会阻塞DML的，这个需要特别小心。 十四、count(*)这么慢，我该怎么办？count(*)的实现方式在不同的MySQL引擎中，count(*)有不同的实现方式。 MyISAM引擎把一个表的总行数存在了磁盘上，因此执行count(*)的时候会直接返回这个数，效率很高； 而InnoDB引擎就麻烦了，它执行count(*)时，需要把数据一行一行地从引擎里面读出来，然后累积计数。 这是需要注意的是，我们在这篇文章里讨论的是没有过滤条件的count()，*如果加入where条件的话，MyISAM表也是不能返回得这么快的**。 为什么InnoDB不跟MyISAM一样，也把数字存起来呢？ 这是因为即使是在同一个时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB表“应该返回多少行”也是不确定的。 InnoDB在执行count( * )操作时的优化，InnoDB是索引组织表，主键索引数的叶子节点是数据，而普通索引树的叶子节点是主键值。所以，普通索引树比主键索引树小很多。对于count(*)这样的操作，遍历哪个索引树得到的结果逻辑上都是一样的。因此，MySQL优化器会找到最小的那棵树来遍历。在保证逻辑正确的前提下，尽量减少扫描的数据量，是数据库系统涉及的通用法则之一。 show table status 里面也有一个 TABLE_ROWS 用于显示这个表当前有多少行，这个命令执行挺快的，那这个 TABLE_ROWS 能代替 count(*) 吗？答案是不能。 因为TABLE_ROWS是从采样估算得来的，误差可能达到40%到50%。所以，show table status命令显示的行数不能直接使用。 MyISAM 表虽然 count() 很快，但是不支持事务；show table status 命令虽然返回很快，但是不准确；InnoDB 表直接 count() 会遍历全表，虽然结果准确，但会导致性能问题。 用缓存系统保存计数 对于更新很频繁的库来说，可能会第一个想到，用缓存系统来支持。 可以使用一个Redis服务来保存这个表的总行数。这个表每被插入一行Redis计数就加1，每被删除一行Redis计数就减1.。 但这种方式仍存在问题，缓存系统可能会丢失更新。 Redis数据不能永久的留在内存里，所以需要找一个地方把这个值定期地持久化存储起来。但即使这样仍可丢失更新。（Redis操作完后还没有持久化存储起来就异常重启了，重启后就会从持久化存储的地方把值读回来，这时前面的操作就丢失了。 Redis异常重启后，可以到数据库里面单独执行一次count()获取真实的行数，再把这个值写回到Redis里就可以了。这样就可以解决*缓存系统丢失更新问题。** 但实际上，将计数保存在缓存系统中的方式，还不只是丢失更新的问题。即使Redis正常工作，这个值还是逻辑上不精确的。（有可能插入一行数据后，Redis计数还没加1，另一个线程就读取了Redis计数和实际行数；有可能Redis计数已经加1了，但还没插入数据，另一个线程就已经开始读取了。） 在数据库保存计数（较好）根据上面的分析，用缓存系统保存计数有丢失和计数不精确的问题。如果把这个计数直接放到数据库里单独的一张计数表C中，会怎么呢? 首先，这解决了崩溃丢失的问题，InnoDB时支持崩溃恢复不丢数据的。 通过事务解决上述的逻辑不精确问题。 把计数放在Redis里面，不能保证计数和MySQL表里的数据精确一致的原因，是这两个不同的存储构成的系统，不支持分布式事务，无法拿到精确一致的视图。而把计数值也放在MySQL中，就解决了一致性视图的问题。 不同的count用法（基于InnoDB引擎）count(*)、count(主键 id)、count(字段)和count(1)等不同用法的性能有什么差别？ count()是一个聚合函数，对于返回的结果集，一行行地判断，如果count函数的参数不是NULL，累计值就加1，否则不加。最后返回累计值。 所以，count(*)、count(主键 id)和count(1)都表示返回满足条件的结果集的总行数；而count(字段)，则表示返回满足条件的数据行里面，参数“字段不为NULL的总个数。 至于分析性能差别的时候，有几个原则： server层要什么就给什么； InnoDB只给必要的值； 现在的优化器只优化了count(*)的语义为”取行数“，其他”显而易见“的优化并没有做。 对于count（主键 id）来说，InnoDB引擎会遍历整张表，把每一行的id值都取出来，返回给server层。server层拿到id后，判断是不可能为空的，就按行累加。 对于count（1）来说，InnoDB引擎遍历整张表，但不取值。server层对于返回的每一行，放一个数字”1“进去，判断是不可能为空的，按行累加。 单看这两个用法，可以看出count(1)执行得要比count(主键 id)快。因为，从引擎返回id会涉及到解析数据行，以及拷贝字段值得操作。 对于count(字段)来说： 如果这个”字段“是定义为not null的话，一行行地从记录中读出这个字段，判断不能为null，按行累加。 如果这个字段”定义允许为null，那么执行的时候，判断到有可能是null，还要把值取出来在判断一下，不是null才累加。 但是count(*)是例外,并不会把全部字段取出来，而是专门做了优化，不去之。count(*)肯定不是null，按行累加。 结论：按照效率排序的话，count(字段)&lt;count(主键 id)&lt;count(1)约等于count(*),所以建议尽量使用count( * ) 思考题用一个计数表记录一个业务表的总行数，在往业务表插入数据的时候，需要给计数值加1. 逻辑实现上是启动一个事务，执行两个语句： insert into 数据表； update 计数表,计数值加1. 从系统并发能力的角度考虑，怎么安排这两个语句的顺序。 并发系统性能的角度考虑，应该先插入操作记录，再更新计数表。 因为更新计数表涉及到行锁的竞争，先插入在更新能最大程度地减少事务之间的锁等待，提升并发度。 在更新计数表时，一定会传入where table_name=$table_name,使用主键索引，更新加行锁只会锁在一行上。 而在不同业务表插入数据，是更新不同的行，不会有行锁。 十五、日志和索引相关问题日志相关问题两阶段提交：写入redo log处于prepare阶段–&gt;写binlog–&gt;redo log进行commit 若在写完binlog后，redo log还没commit前发生crash，那崩溃恢复时MySQL会怎么处理？ 崩溃恢复的判断规则： 如果redo log里面的事务是完整的，也就是已经有了commit标识，则直接提交； 如果redo log里面的事务只有完整的prepare，则判断对应的事务binlog是否存在并完整 ​ a. 如果是，则提交事务； ​ b. 否则，回滚事务。 追问1：MySQL怎么知道binlog是完整的？ 回答：一个事务的binlog是有完整格式的： ​ statement格式的binlog，最后会有COMMIT； ​ row格式的binlog，最后会有一个XID event。 另外，在MySQL5.6.2以后，还引入了binlog-checksum参数，用来验证binlog内容的正确性。 追问2：redo log和binlog是怎么关联起来的？ 回答：他们有一个共同的数据字段，叫XID。崩溃恢复时，会按顺序扫描redo log： ​ 如果碰到既有prepare、又有commit的redo log，就直接提交； ​ 如果碰到只有prepare、而没有commit的redo log，就拿着XID去binlog找对应的事务。 追问3：处于prepare阶段的redo log加上完整的binlog，重启就能恢复，MySQL为什么要这么设计？ 回答：在binlog写完后发生崩溃，这时候binlog已经写入了，之后就会被从库（或者用这个binlog恢复出来的库）使用。 所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性。 追问4：如果这样的话，为什么还要两阶段提交呢？干脆限redo log写完，再写binlog。崩溃恢复时，必须得两个日志都完整才可以。是不是一样的逻辑？ 回答：两阶段提交是经典的分布式系统问题，并不是MySQL独有的。 例如，事务的持久性问题。 对于InnoDB引擎来说，如果redo log提交完成了，事务就不能回滚（如果这还允许回滚，就可能覆盖掉别的事务的更新）。而如果redo log直接提交，然后binlog写入的时候失败，InnoDB又回滚不了，数据和binlog日志又不一致了。 两阶段提交就是为了给所有人一个机会，当每个人都说“我ok”的时候，再一起提交。 追问5：不引入两个日志，也就没有两阶段提交的必要了。只用binlog来支持崩溃恢复，又能支持归档，不就可以了？ 回答：binlog的能力还不能支持崩溃恢复。 追问6：那能不能反过来，只用redo log，不要binlog？ 回答：如果只从崩溃恢复的角度来讲是可以的。但binlog有着一些redo log无法替代的功能。 一个是归档。redo log是循环写，写到末尾要回到开头继续写，这样历史日志无法保留，redo log也就起不到归档的作用。 一个就是MySQL系统依赖于binlog。binlog作为MySQL一开始就有的功能，被用在了很多地方。其中，MySQL系统高可用的基础，就是binlog复制。 追问7：redo log一般设置多大？ 回答：redo log太小的话，会导致很快就被写满，然后不得不强行刷redo log，这样WAL机制的能力就发挥不出来了。 所以，如果是现在常见的几个TB的磁盘的话，就不要调小气了，直接将redo log设置为4个文件，每个文件1GB吧。 追问8：正常运行的实例，数据写入后的最终落盘，是从redo log更新过来的还是从buffer pool更新过来的呢？ 回答：实际上redo log并没记录数据页的完整数据，所以它并没有能力自己去更新磁盘数据页，也就不存在“数据最终落盘，是由redo log更新过去”的情况。 如果是正常运行的实例的话，数据页被修改以后，跟磁盘的数据页不一致，称为脏页。最终数据落盘，就是把内存中的数据页写盘。这个过程，甚至与redo log毫无关系。 在崩溃恢复场景中，InnoDB如果判断到一个数据页可能在崩溃恢复时丢失了更新，就会将它读到内存，然后让redo log更新内存内容。更新完成后，内存页变成脏页，就回到了第一种情况的状态。 追问9：redo log buffer是什么？是先修改内存，还是险些redo log文件？ 回答：在一个事务的更新过程中，日志是要写多次的。 1234begin;insert into t1 ...insert into t2 ...commit; 这个事务要往两个表中插入记录，插入数据的过程中，生成的日志都得先保存起来，但又不能在还没commit的时候就直接写到redo log文件里。 所以，redo log buffer就是一块内存，用来先存redo日志的。也就是说，在执行第一个insert的时候，数据的内存被修改了，redo log buffer也写入了日志。真正把日志写到redo log文件（文件名是ib_logfile+数字），是在执行commit语句的时候做的。 业务设计问题 业务上有这样的需求，A、B两个用户，如果互相关注，则成为好友。设计上是有两张表，一个是like表，一个是friend表，like表有user_id、liker_id两个字段，我设置为符合唯一索引即uk_user_id_liker_id。语句执行逻辑是这样的： 以A关注B为例 第一步，先查询对方有没有关注自己（B有没有关注A） select * from like where user_id = B and liker_id = A; 如果有，则成为好友 insert into friend; 没有，则只是单向关注关系 insert into like; 但是如果A、B同时关注对方，会出现不会成为好友的情况。因为上面第1步，双方都没关注对方。第1步即使使用了排他锁也不行，因为记录不存在，行锁无法生效。请问这种情况，在MySQL锁层面有没有办法处理？ 1234567CREATE TABLE `like` ( `id` int(11) NOT NULL AUTO_INCREMENT, `user_id` int(11) NOT NULL, `liker_id` int(11) NOT NULL, PRIMARY KEY (`id`), UNIQUE KEY `uk_user_id_liker_id` (`user_id`,`liker_id`) ) ENGINE=InnoDB; CREATE TABLE `friend` ( id` int(11) NOT NULL AUTO_INCREMENT, `friend_1_id` int(11) NOT NULL, `firned_2_id` int(11) NOT NULL, UNIQUE KEY `uk_friend` (`friend_1_id`,`firned_2_id`) PRIMARY KEY (`id`) ) ENGINE=InnoDB; 上述代码为建表sql语句。 首先，要给”like”表增加一个字段，比如叫做relation_ship,并设为整型，取值1、2、3. 值是1的时候，表示user_id关注liker_id; 值是2的时候，表示liker_id关注user_id; 值是3的时候，表示互相关注。 然后，当A关注B的时候，逻辑改成如下这个样子： 应用代码里，比较A和B的大小，如果A&lt;B,就执行下面的逻辑 123456mysql&gt; begin; /* 启动事务 */ insert into `like`(user_id, liker_id, relation_ship) values(A, B, 1) on duplicate key up select relation_ship from `like` where user_id=A and liker_id=B; /* 代码中判断返回的 relation_ship， 如果是 1，事务结束，执行 commit 如果是 3，则执行下面这两个语句： */insert ignore into friend(friend_1_id, friend_2_id) values(A,B); commit; 如果A&gt;B,则执行下面的逻辑 123456mysql&gt; begin; /* 启动事务 */ insert into `like`(user_id, liker_id, relation_ship) values(B, A, 2) on duplicate key up select relation_ship from `like` where user_id=B and liker_id=A; /* 代码中判断返回的 relation_ship， 如果是 2，事务结束，执行 commit 如果是 3，则执行下面这两个语句： */ insert ignore into friend(friend_1_id, friend_2_id) values(B,A); commit; 这个设计里，让“like”表里数据保证user_id&lt;liker_id,这样不论是A关注B，还是B关注A，在操作”like“表的时候，如果反向的关系已经存在，就会出现行锁冲突。 insert … on duplicate语句，确保了在事务内部，执行了这个SQL语句后，就强行占住了这个行锁，之后的select判断relation_ship这个逻辑时就确保了是在行锁保护下的读操作。 操作符“|”是按位或，连同最后一句insert语句里的ignore，是为了保证重复调用时的幂等性。 业务开发保证不会插入重复记录的情况下，着重要解决性能问题的时候，才建议尽量使用普通索引。在无法保证不会插入重复记录时可以使用唯一索引。 思考题当MySQL去更新一行，但是要修改的值跟原来的值是相同的，这时候MySQL会真的去执行一次修改吗？还是看到值相同就直接返回呢？ 答案：InnoDB认真执行了“把这个值修改成（1，2）”这个操作，该加锁的加锁，该更新的更新。 十六、“order by”是怎么工作的？查询城市是“杭州”的所有人的名字，并且按照姓名排序返回前1000个人的姓名、年龄。 SQL语句： 1select city,name,age from t where city=\"杭州\" order by name limit 1000; 下面讲述该语句的执行流程。 全字段排序为避免全表扫描，我们需要在city字段加上索引。 MySQL 会给每个线程分配 一块内存用于排序，称为 sort_buffer。 通常情况下，这个语句执行流程如下所示 ： 初始化 sort_buffer，确定放入 name、city、age 这三个字段； 从索引 city 找到第一个满足 city=’杭州’条件的主键 id，也就是图中的 ID_X； 到主键 id 索引取出整行，取 name、city、age 三个字段的值，存入 sort_buffer 中； 从索引 city 取下一个记录的主键 id； 重复步骤 3、4 直到 city 的值不满足查询条件为止，对应的主键 id 也就是图中的 ID_Y； 对 sort_buffer 中的数据按照字段 name 做快速排序； 按照排序结果取前 1000 行返回给客户端。 “按 name 排序”这个动作，可能在内存中完成，也可能需要使用外部排序，这取决 于排序所需的内存和参数 sort_buffer_size。（sort_buffer_size，就是 MySQL 为排序开辟的内存（sort_buffer）的大小） 通过查看OPTIMIZER_TRACE的结果中的number_of_tmp_files可以看到这个排序语句是否使用了临时文件。 rowid排序上面这个算法过程中，只对原表的数据读了一遍，剩下的操作都是在sort_buffer和临时文件中执行的。如果查询要返回的字段很多的话，那么sort_buffer里面要放的字段数太多，这样内存里能够同时放下的行数很少，要分成很多个临时文件，排序的性能会很差。 如果用于排序的行数据的长度超过max_length_for_sort_data的值，则MySQL认为单行长度太大。如果MySQL认为排序的单行长度太大会换一个算法，新的算法放入sort_buffer的字段，只有要排序的列和主键id。 参与排序的只有要排序的列和主键id。 全字段排序 VS rowid排序当MySQL是在担心排序内存太小，会影响排序效率，才会采用rowid排序算法。rowid排序只会返回要排序的列和主键id，一般需要再回到原表去取数据。 当MySQL认为内存足够大，会优先选择全字段排序，把需要的字段都放到sort_buffer中。 这也体现了MySQL的一个设计思想：如果内存够，就要多利用内存，尽量减少磁盘访问。 对于InnoDB表来说，rowid排序会要求回表多造成磁盘读，因此不会被优先选择。 十七、如何正确地显示随机消息？设计一个英语学习 App 首页有一个随机显示单词的功能，也就是根据每个用户的级别有一个单词表，然后这个用户每次访问首页的时候，都会随机滚动显示三个单词。他们发现随着单词表变大，选单词这个逻辑变得越来越慢，甚至影响到了首页的打开速度。 这个表的建表语句和初始数据的命令如下： 1234567891011121314151617mysql&gt; CREATE TABLE `words` ( `id` int(11) NOT NULL AUTO_INCREMENT, `word` varchar(64) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB; delimiter ;; create procedure idata() begin declare i int; set i=0; while i&lt;10000 do insert into words(word) values(concat(char(97+(i div 1000)), char(97+(i % 1000 div 1 set i=i+1; end while;end;;delimiter ; call idata(); 为了量化说明，往里面加入了10000行记录。 内存临时表首先，会想到用order by rand()来实现这个逻辑。 1mysql&gt;select word from words order by rand() limit 3; 可以用explain命令来看看这个语句的执行情况。 Extra字段显示Using temporary，表示的是需要使用临时表；Using filesort,表示的是需要执行排序操作。即需要临时表，并且需要在临时表上排序。 Extra字段的Using index，表示的是使用了覆盖索引。 对于InnoDB表来说，执行全字段排序会减少磁盘访问，因此会被优先选择。 对于内存表，回表过程只是简单的根据数据行的位置，直接访问内存得到数据，根本不会导致多访问磁盘。 临时内存表的排序来说，因为此时优化器没有了多访问磁盘的顾虑，所以他会优先考虑的是用于排序的行越小越好，因此会优先选择rowid排序。 这条语句的执行流程是这样的： 创建一个临时表，这个临时表使用的是memory引擎 ，表里有两个字段，第一个字段是double类型，为了后面描述方便，记为字段R，第二个字段是varchar(64)类型，记为字段W。并且，这个表没有建索引。 从word表中，按主键顺序取出所有的word值。对于每一个word值，调用rand()函数生成一个大于0小于1的随机小数，并把这个随机小数和 word 分别存入临时表的 R 和 W 字段中，到此，扫描行数是 10000。 现在临时表有 10000 行数据了，接下来你要在这个没有索引的内存临时表上，按照字 段 R 排序。 初始化 sort_buffer。sort_buffer 中有两个字段，一个是 double 类型，另一个是整 型。 从内存临时表中一行一行地取出 R 值和位置信息（我后面会和你解释这里为什么是“位 置信息”），分别存入 sort_buffer 中的两个字段里。这个过程要对内存临时表做全表 扫描，此时扫描行数增加 10000，变成了 20000。 在 sort_buffer 中根据 R 的值进行排序。注意，这个过程没有涉及到表操作，所以不会 增加扫描行数。 排序完成后，取出前三个结果的位置信息，依次到内存临时表中取出 word 值，返回给 客户端。这个过程中，访问了表的三行数据，总扫描行数变成了 20003。 通过慢查询日志（slow log）来验证一下我们分析得到的扫描行数是否正确。 InnoDB表如果没有主键，那么InnoDB会自己生成一个长度为6字节的rowid来作为主键。每个引擎都有用来唯一标识数据行的信息，InnoDB的是rowid。 MEMORY引擎不是索引组织表，可以认为它是一个数组，因此，这个rowid其实就是数组的下标。 order by rand()使用了内存临时表，内存临时表排序时使用了rowid排序方法。 如果直接使用order by rand()，这个语句需要Using temporary和Using filesort，查询的执行代价往往是比较大的。所以，在设计时要尽量避开这种写法。 磁盘临时表并不是所有的临时表都是内存表。tmp_table_size这个参数限制了内存临时表的大小，默认值是16M。如果临时表大小超过了tmp_table_size，那么内存临时表就会转成磁盘临时表。 磁盘临时表使用的引擎默认是InnoDB，是由参数internal_tmp_disk_storage_engine控制的。 当使用磁盘临时表时，对应的就是一个没有显示索引的InnoDB表的排序过程。 优先队列排序算法，不需要用临时文件。 无论是使用哪种类型的临时表，order by rand()这种写法都会让计算过程非常复杂，需要大量的扫描行数，因此排序过程的资源消耗也会很大。 随机排序方法如果只随机选择一个word值，思路是这样的： 取得这个表的主键id的最大值M和最小值N； 用随机函数生成一个最大值到最小值之间的数X=(M-N)*rand() + N; 取不小于X的第一个ID的行。 这个方法效率很高，因为取max(id)和min(id)都是不需要扫描索引的，而第三步的select也可以用索引快速定位，可以认为就只扫描了3行。但这个方法因为ID中间可能有空洞，因此选择不同行的概率不一样，不是真正的随机。 为了得到严格随机的结果，可以用下面的流程： 取得整个表的行数，并记为C。 取得Y=floor(C*rand())。floor函数在这里的作用，就是取整数部分。 再用limit Y,1取得一行。 十八、为什么这些SQL语句逻辑相同，性能却差异巨大？案例一：条件字段函数操作假设你现在维护了一个交易系统，其中交易记录表tradelog包含交易流水号（tradeid）、交易员id（operator）、交易时间（t_modified）等字段。这个表的建表语句如下： 12345678mysql&gt;CREATE TABLE 'tradelog'( 'id' int(11) NOT NULL, 'tradeid' varchar(32) DEFAULT NULL, 'operator' int(11) DEFAULT NULL, PRIMARY KEY ('id'), KEY 'tradeid' ('tradeid'), KEY 't_modified' ('t_modified'))ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 假设，现在已经记录了从 2016 年初到 2018 年底的所有数据，运营部门有一个需求是， 要统计发生在所有年份中 7 月份的交易记录总数。这个逻辑看上去并不复杂，你的 SQL 语句可能会这么写： 1mysql&gt; select count(*) from tradelog where month(t_modified)=7; 由于 t_modified 字段上有索引，于是你就很放心地在生产库中执行了这条语句，但却发现执行了特别久，才返回了结果。 如果对字段做了函数计算，就用不上索引了，这是MySQL的规定。 对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。 需要注意的是，优化器并不是要放弃使用这个索引。在这个例子里，放弃了树搜索功能，优化器可以选择遍历主键索引，也可以选择遍历索引 t_modified，优化器对比索引大小后发现，索引 t_modified 更小，遍历这个索引比遍历 主键索引来得更快。因此最终还是会选择索引 t_modified。 由于加了month()函数操作，MySQL无法再使用索引快速定位功能，而只能使用全索引扫描。 案例二：隐式类型转换1mysql&gt;select * from tradelog where tradeid=110717; 交易编号tradeid这个字段上，本来就有索引，但是explain的结果却显示，这条语句需要走全表扫描。tradeid的字段类型是varchar(32)，而输入的参数却是整型，所以需要做类型转换。 现在这里就有两个问题： 数据类型转换的规则是什么？ 在MySQL中，字符串和数字做比较的话，是将字符串转换成数字。 为什么有数据类型转换，就需要走全索引扫描？ 做数据类型转换要对索引字段做函数操作，优化器会放弃走树搜索功能。 案例三：隐式字符编码转换假设系统里还有另外一个表trade_detail,用于记录交易的操作细节。为了便于量化分析和复现，我往交易日志表tradelog和交易详情表trade_detail这两个表里插入一些数据。 1234567891011121314151617181920212223mysql&gt;CREATE TABLE 'trade_detail'( 'id' int(11) NOT NULL, 'tradeid' varchar(32) DEFAULT NULL,/*操作步骤*/ 'step_info' varchar(32) DEFAULT NULL, /* 步骤信息 */ PRIMARY KEY (`id`), KEY `tradeid` (`tradeid`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; insert into tradelog values(1, 'aaaaaaaa', 1000, now()); insert into tradelog values(2, 'aaaaaaab', 1000, now()); insert into tradelog values(3, 'aaaaaaac', 1000, now()); insert into trade_detail values(1, 'aaaaaaaa', 1, 'add'); insert into trade_detail values(2, 'aaaaaaaa', 2, 'update');insert into trade_detail values(3, 'aaaaaaaa', 3, 'commit'); insert into trade_detail values(4, 'aaaaaaab', 1, 'add'); insert into trade_detail values(5, 'aaaaaaab', 2, 'update'); insert into trade_detail values(6, 'aaaaaaab', 3, 'update again'); insert into trade_detail values(7, 'aaaaaaab', 4, 'commit'); insert into trade_detail values(8, 'aaaaaaac', 1, 'add'); insert into trade_detail values(9, 'aaaaaaac', 2, 'update'); insert into trade_detail values(10, 'aaaaaaac', 3, 'update again'); insert into trade_detail values(11, 'aaaaaaac', 4, 'commit'); 这时候，如果要查询id=2的交易的所有操作步骤信息，SQL语句可以这么写： 1mysql&gt;select d.* from tradelog l,trade_detail d where d.tradeid=l.tradeid and l.id=2; 我们一起来看下这个结果： 第一行显示优化器会先在交易记录表 tradelog 上查到 id=2 的行，这个步骤用上了主 键索引，rows=1 表示只扫描一行； 第二行 key=NULL，表示没有用上交易详情表 trade_detail 上的 tradeid 索引，进行 了全表扫描。 在这个执行计划里，是从 tradelog 表中取 tradeid 字段，再去 trade_detail 表里查询匹 配字段。因此，我们把 tradelog 称为驱动表，把 trade_detail 称为被驱动表，把 tradeid 称为关联字段。 执行流程： 第 1 步，是根据 id 在 tradelog 表里找到 L2 这一行；第 2 步，是从 L2 中取出 tradeid 字段的值；第 3 步，是根据 tradeid 值到 trade_detail 表中查找条件匹配的行。explain 的结果里 面第二行的 key=NULL 表示的就是，这个过程是通过遍历主键索引的方式，一个一个 地判断 tradeid 的值是否匹配。 进行到这里，你会发现第 3 步不符合我们的预期。因为表 trade_detail 里 tradeid 字段上 是有索引的，我们本来是希望通过使用 tradeid 索引能够快速定位到等值的行。但，这里并没有。 因为这两个表的字符集不同，一个是 utf8， 一个是 utf8mb4，所以做表连接查询的时候用不上关联字段的索引。但为什么字符集不同就用不上索引呢？ 字符集 utf8mb4 是 utf8 的超集，所以当这两个 类型的字符串在做比较的时候，MySQL 内部的操作是，先把 utf8 字符串转成 utf8mb4 字符集，再做比较。 这就再次触发了我们上面说到的原则：对索引字段做函数操作，优化器会放弃走树搜索功能。 思考题表结构如下： 123456mysql&gt; CREATE TABLE `table_a` ( `id` int(11) NOT NULL, `b` varchar(10) DEFAULT NULL, PRIMARY KEY (`id`), KEY `b` (`b`)) ENGINE=InnoDB; 假设现在表里面，有 100 万行数据，其中有 10 万行数据的 b 的值是’1234567890’， 假设现在执行语句是这么写的: 1mysql&gt; select * from table_a where b='1234567890abcd'; 这时候，MySQL 会怎么执行呢？最理想的情况是，MySQL 看到字段 b 定义的是 varchar(10)，那肯定返回空呀。可惜， MySQL 并没有这么做。那要不，就是把’1234567890abcd’拿到索引里面去做匹配，肯定也没能够快速判断出 索引树 b 上并没有这个值，也很快就能返回空结果。但实际上，MySQL 也不是这么做的。这条 SQL 语句的执行很慢，流程是这样的： 在传给引擎执行的时候，做了字符截断。因为引擎里面这个行只定义了长度是 10，所 以只截了前 10 个字节，就是’1234567890’进去做匹配； 这样满足条件的数据有 10 万行； 因为是 select *， 所以要做 10 万次回表； 但是每次回表以后查出整行，到 server 层一判断，b 的值都不 是’1234567890abcd’; 返回结果是空。 十九、为什么我只查一行的语句，也执行这么慢为了便于描述，构造一个表，基于这个表来说明今天的问题。这个表有两个字段id和c，并且在里面插入了10万行记录。 12345678910111213141516171819mysql&gt;CREATE TABLE 't'( 'id' int(11) NOT NULL, 'c' int(11) DEFAULT NULL, PRIMARY KEY ('id'))ENGINE=InnoDB;delimiter ;;create procedure idata()begin declare i int; set i=1; while(i&lt;=100000)do insert into t values(i,i); set i=i+1; end while;end;;delimiter ;call idata(); 第一类：查询长时间不返回如图1所示，在表t执行下面的SQL语句： 1mysql&gt;select * from t where id=1; 查询结果长时间不返回。 一般碰到这种情况的话，大概率是表t被锁住了。接下来分析原因的时候，一般都是首先执行一下show processlist命令，看看当前语句处于什么状态。 然后再针对每种状态，去分析他们产生的原因，如何复现，以及如何处理。 等MDL锁使用show processlist命令查看Waiting for table metadata lock。 MySQL5.7修改了MDL的加锁策略。 在MySQL5.7版本下的简单复现： session A通过lock table命令持有表t的MDL写锁，而session B的查询需要获取MDL读锁。所以，session B进入等待状态。 这类问题的处理方式，就是找到谁持有MDL写锁，然后把它kill掉。 通过查询sys.schema_table_lock_waits这张表，我们就可以直接找出造成阻塞的process id，把这个连接用kill命令断开即可。 等flush另外一种查询被堵住的情况。 例如在表t上，执行下面的SQL语句： 1mysql&gt; select * from information_schema.processlist where id=1; 这是查出这个线程的状态是Waiting for table flush，表示有一个线程要对表t做flush操作。MySQL中对表做flush操作的用法，一般有以下两个： 123flush tables t with read lock;/*只关闭表t*/flush tables with read lock;/*关闭MySQL中所有表*/ 出现Waiting for table flush状态的可能情况是：有一个flush tables命令被别的语句堵住了，然后它又堵住了我们的select语句。 等行锁如果有一个事务在这行记录上持有一个写锁，还不提交，我们的select语句就会被堵住。如果用的是MySQL5.7版本，可以通过sys.innodb_lock_waits表查到。 第二类：查询慢扫描行数多，所以执行慢只扫描一行，但是执行很慢 session A先用transaction with consistent snapshot命令启动了一个事务，之后session B才开始执行update语句。 带lock in share mode的SQL语句，是当前读，因此会直接读到1000001这个结果，所以速度很快，而select * from t where id=1这个语句，是一致性读，因此需要从1000001开始，依次执行undo log，执行了100万次以后，才将1这个结果返回。 MDL锁与实现MySQL5.5引入了MDL锁（metadata lock），用于解决或保证DDL操作与DML操作之间的一致性。 Session A Session B BEGIN; SELECT * FROM XXX DROP TABLE XXX SELECT * FROM XXX 若没有MDL锁的保护，则Session B可以直接执行DDL操作，导致Session A出错。 MDL锁是在Server中实现。另外，MDL锁还能实现其他粒度级别的锁，比如全局锁、库级别的锁、表空间级别的锁，这是InnoDB存储引擎层不能直接实现的锁。 但与InnoDB锁的实现一样，MDL锁也是类似一颗树的各个对象从上至下进行加锁（对锁进行加锁具体见：《MySQL技术内幕：InnoDB存储引擎》） 目前MDL有如下锁模式，锁之间的兼容性可见源码mdl.cc: 锁模式 对应SQL MDL_INTENTION_EXCLUSIVE GLOBAL对象、SCHEMA对象操作会加此锁 MDL_SHARED FLUSH TABLES with READ LOCK MDL_SHARED_HIGH_PRIO 仅对MyISAM存储引擎有效 MDL_SHARED_READ SELECT查询 MDL_SHARED_WRITE DML语句 MDL_SHARED_WRITE_LOW_PRIO 仅对MyISAM存储引擎有效 MDL_SHARED_UPGRADABLE ALTER TABLE MDL_SHARED_NO_WRITE FLUSH TABLES xxx,yyy,zzz READ MDL_SHARED_READ_ONLY ALTER TABLE MDL_SHARED_NO_READ_WRITE FLUSH TABLE xxx WRITE MDL_EXCLUSIVE ALTER TABLE xxx PARTITION BY … 二十、幻读是什么，幻读有什么问题？为了便于说明问题，使用一个小一点儿的表。建表和初始化语句如下： 123456789CREATE TABLE 't'( 'id' int(11) NOT NULL, 'c' int(11) DEFAULT NULL, 'd' int(11) DEFAULT NULL, PRIMARY KEY('id'), KEY 'c' ('c'))ENGINE=InnoDB;insert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25); 下面的语句序列，是怎么加锁的，加的锁又是什么时候释放的呢？ 123begin;select * from t where d=5 for update;/*当前读*/commit; 比较好理解的是，这个语句会命中 d=5 的这一行，对应的主键 id=5，因此在 select 语句 执行完成后，id=5 这一行会加一个写锁，而且由于两阶段锁协议，这个写锁会在执行 commit 语句的时候释放。由于字段 d 上没有索引，因此这条查询语句会做全表扫描。那么，其他被扫描到的，但是 不满足条件的 5 行记录上，会不会被加锁呢？ 幻读是什么？幻读指的是一个事务在前后两次查询同一个范围时，后一次查询看到了前一次查询没有看到的行。 说明： 在可重复读隔离级别下，普通的查询是快照读，是不会看到别的事务插入的数据的。因此，幻读在“当前读”下才会出现。 上面session B的修改结果，被session A之后的select语句用“当前读”看到，不能称为幻读。幻读仅专指”新插入的行“。 查询加了for update或lock in share mode的，都是当前读。当前读的规则：读所有已经提交的记录的最新值。 幻读有什么问题？ update 的加锁语义和 select …for update 是一致的，所以这时候加上这条 update 语句 也很合理。session A 声明说“要给 d=5 的语句加上锁”，就是为了要更新数据，新加的这条 update 语句就是把它认为加上了锁的这一行的 d 值修改成了 100。现在，我们来分析一下上图执行完成后，数据库里会是什么结果。 经过 T1 时刻，id=5 这一行变成 (5,5,100)，当然这个结果终是在 T6 时刻正式提交 的 ; 2. 经过 T2 时刻，id=0 这一行变成 (0,5,5); 3. 经过 T4 时刻，表里面多了一行 (1,5,5); 4. 其他行跟这个执行序列无关，保持不变。这样看，这些数据也没啥问题，但是我们再来看看这时候 binlog 里面的内容。 T2 时刻，session B 事务提交，写入了两条语句； 2. T4 时刻，session C 事务提交，写入了两条语句； 3. T6 时刻，session A 事务提交，写入了 update t set d=100 where d=5 这条语句。我统一放到一起的话，就是这样的： 即使把所有的记录都加上锁，还是阻止不了新插入的记录，这也是为什么“幻 读”会被单独拿出来解决的原因。 如何解决幻读？产生幻读的原因是，行锁只能锁住行，但是新插入记录这个动作，要更新的是记录之间的“间隙”。因此，为了解决幻读问题，InnoDB只好引入新的锁，也就是间隙锁（Gap Lock）。 间隙锁间隙锁，锁的就是两个值之间的空隙。 跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作。间隙锁之间都不存在冲突关系。 间隙锁和行锁合称next-key lock，每个next-key lock是前开后闭区间。 间隙锁和next-key lock的引入，帮我们解决了幻读的问题，但同时也带来了一些“困扰”。 间隙锁的引入，可能会导致同样的语句锁住更大的范围，这其实是影响了并发度的。 间隙锁是在可重复读隔离级别下才会生效的。其实读提交隔离级别在外键场景下还是有间隙锁，相对比较复杂。 读提交隔离级别下，锁的范围更小，锁的时间更短，这也是不少业务都默认使用读提交隔离级别的原因。 二十一、为什么我只改了一行的语句，锁这么多？加锁规则，包含两个“原则”、两个“优化”和一个“bug”。 原则1：加锁的基本单位是next-key lock。next-key lock是前开后闭区间。 原则2：查找过程中访问到的对象才会加锁。 优化1：索引上的等值查询，给唯一索引加锁的时候，next-key lock退化为行锁。 优化2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock退化成间隙锁。 一个bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。 案例一：等值查询间隙锁 案例二：非唯一索引等值锁 案例三：主键索引范围锁 案例四：非唯一索引范围锁 案例五：唯一索引范围锁bug 案例六：limit语句加锁 可以看到，（c=10,id=30）之后的这个间隙并没有在加锁范围里，因此insert语句插入c=12是可以执行成功的。 在删除数据的时候尽量加limit。 案例七：一个死锁的例子 分析加锁规则时可以用next-key lock来分析。但是要知道在具体执行时，是要分成间隙锁和行锁两段来执行的。 二十二、MySQL有哪些提高性能的方法？业务高峰期，生产环境的MySQL压力太大，没法正常响应，需要短期内、临时性地提升一些性能。 下面讲一下这些临时方案，并着重说一说他们可能存在的风险。 短连接风暴如果使用的是短连接，在业务高峰期的时候，就可能出现连接数突然暴涨的情况。 短连接模型存在一个风险，就是一旦数据库处理得慢一些，连接数就会暴涨。max_connections参数，用来控制一个MySQL实例同时存在的连接数的上限，超过这个值，系统就会拒绝接下来的连接请求，对于被拒绝连接的请求来说，从业务角度看就是数据库不可用。 碰到这种情况，有两种处理方法： 第一种方法：先处理掉那些占着连接但是不工作的线程。 max_connections的计算，不是看谁在running，是只要连着就占用一个计数位置。对于那些不需要保持的链接，我们可以通过kill connection主动踢掉。这个行为跟事先设置wait_timeout这么多秒之后，就会被MySQL直接断开链接。 但是需要注意，在show processlist的结果里，踢掉显示为sleep的线程，可能是有损的。优先断开事务外空闲的连接。如果还不够再考虑断开事务内空闲太久的连接。 从数据库端主动断开连接可能是有损的，尤其是有的应用端收到这个错误后，不重新连接，而是直接用这个已经不能用的句柄重试查询。 第二种方法：减少连接过程的消耗 如果现在数据库确认是被连接行为打挂了，那么一种可能的做法，是让数据库跳过权限验证阶段。 跳过权限验证的方法是：重启数据库，并使用-skip-grant-tables参数启动。这样，整个MySQL会跳过所有的权限验证阶段，包括连接过程和语句执行过程在内。该方法风险极高。 查询问题比较典型的有两类，一类是有新出现的慢查询导致的，一类是由QPS（每秒查询数）突增导致的。 慢查询性能问题在MySQL中，会引发性能问题的慢查询，大体有以下三种可能： 索引没有设计好； 一般通过紧急创建索引来解决，对于那种高峰期数据库已经被这个语句打挂了的情况，最高效的做法就是直接执行alter table语句。 比较理想的是能够在备库先执行。假设你现在的服务是一主一备，主库A、备库B，这个方案（在需要紧急处理时效率最高）的大致流程是这样的： ​ （1）在备库B上执行set sql_log_bin=off,也就是不写binlog,然后执行alter table语句加上索引； ​ （2）执行主备切换； ​ （3）这时候主库是B，备库是A。在A上执行set sql_log_bin=off，然后执行alter table语句加上索引。 SQL语句没写好； 通过改写SQL语句来处理，MySQL5.7提供了query_rewrite功能，可以把输入的一种语句改写成另一种模式。 MySQL选错了索引。 应急方案就是给这个语句加上force index。 同样地，使用查询重写功能，给原来的语句加上force index，也可以解决这个问题。 实际上出现最多的是前两种：索引没设计好和语句没写好。通过下面这个过程，我们就可以预先发现问题： 上线前，在测试环境，把慢查询日志（slow log）打开，并且把long_query_time设置为0，确保每个语句都会被记录入慢查询日志； 在测试表里插入模拟线上的数据，做一遍回归测试； 观察慢查询日志里每类语句的输出，特别留意Rows_examined字段是否与预期一致。 可以使用开源工具pt-query-digest检查所有的SQL语句的返回结果。 QPS突增问题最理想的情况是让业务把这个功能下掉，服务自然就会恢复。下掉一个功能，如果从数据库端处理的话，对应于不同的背景，有不同的方法可用。我们这里再和你展开说明一下。 一种是由全新业务的bug导致的。假设你的DB运维是比较规范的，也就是说白名单是一个个假的。这种情况下，如果你能够确定业务方会下掉这个功能，只是时间上没那么快，那么就可以从数据库端直接把白名单去掉。 如果这个新功能使用的是单独的数据库用户，可以用管理员账号把这个用户删掉，然后断开现有连接。这样，这个新功能连接不成功，由它引发的QPS就会变成0. 如果这个新增功能跟主体功能是部署在一起的，那么我们只能通过处理语句来限制。这时，我们可以使用上面提到的查询重写功能，把压力最大的SQL语句直接重写成“select 1”返回。 这个操作的风险很高，可能存在两个副作用： 如果别的功能里面也用到了这个SQL语句模板，会有误伤； 很多业务并不是靠这一个语句就能完成逻辑的，所以如果单独把这一个语句以select 1的结果返回的话，可能会导致后面的业务逻辑一起失败。 所以，方案3是用于止血的，跟前面提到的去掉权限验证一样，应该是你所有选项里优先级最低的一个方案。 本章提到的解决方法主要集中在server层。 二十三、MySQL是怎么保证数据不丢的？只要redo log和binlog保证持久化到磁盘，就能确保MySQL异常重启后，数据可以恢复。 binlog的写入机制binlog的写入逻辑：事务执行过程中，先把日志写到binlog cache（每个线程一个），事务提交的时候，再把binlog cache写到binlog文件中，并清空binlog cache。 一个事务的binlog是不能被拆开的，因此无论这个事务多大，都要保证一次性写入。这就涉及到了binlog cache的保存问题。 参数binlog_cache_size用于控制单个线程内binlog cache所占的内存大小。如果超过了这个参数规定的大小，就暂存到磁盘。 图中的write指的是把日志写入到文件系统的page cache，并没有把数据持久化到磁盘，所以速度比较快。 图中的fsync，才是将数据持久化到磁盘的操作，一般情况下，我们认为fsync才占磁盘的IOPS。 write和fsync的时机，是由参数sync_binlog控制的： sync_binlog=0的时候，表示每次提交事务都只write，不fsync； sync_binlog=1的时候，表示每次提交事务都会执行fsync； sync_binlog=N（N&gt;1）的时候，表示每次提交事务都write，但累积N个事务后才fsync。 在出现IO瓶颈的场景里，将sync_binlog设置成一个比较大的值，可以提升性能。一般不建议把这个参数设成0。 将sync_binlog设置为N，对应的风险是：如果主机发生异常重启，会丢失最近N个事务的binlog日志。 redo log的写入机制redo log可能存在的三种存储状态： 存在redo log buffer中，物理上是在MySQL进程内存中； 写到磁盘（write），但是没有持久化（fsync），物理上是在文件系统的page cache里面； 持久化到磁盘，对应的是hard disk。 为了控制redo log的写入策略，InnoDB提供了innodb_flush_log_at_trx_commit参数，它有三种可能取值： 设置为0时，表示每次事务提交时都只是把redo log留在redo log buffer中；（不建议） 设置为1时，表示每次事务提交时都将redo log直接持久化到磁盘；（常用） 设置为2时，表示每次事务提交时都只是把redo log写到page cache。 事务还没提交的时候，redo log buffer 中的部分日志有可能被持久化到磁盘。 InnoDB 有一个后台线程，每隔 1 秒，就会把 redo log buffer 中的日志，调用 write 写 到文件系统的 page cache，然后调用 fsync 持久化到磁盘。 除了后台线程每秒一次的轮询操作外，还有两种场景会让一个没有提交的事务的redo log写入到磁盘中。 一种是，redo log buffer占用的空间即将达到innodb_log_buffer_size一半的时候，后台线程会主动写盘。由于这个事务并没有提交，所以这个写盘动作只是write，没有调用fsync，也就是只留在了文件系统的page cache。 另一种是，并行的事务提交时，顺带将这个事务的redo log buffer持久化到磁盘。 通常为我们说MySQL的“双1”配置，指的就是sync_binlog和innodb_flush_log_at_trx_commit都设置成1。也就是说，一个事务完整提交前，需要等待两次刷盘，一次是redo log（prepare阶段），一次是binlog。 这意味着从MySQL看到的TPS是每秒两万的话，每秒就会写四万次磁盘。但是，用工具测试出来，磁盘能力也就两万左右，怎么实现两万的TPS？ 解释这个问题，就要用到组提交（group commit）机制了。这里，我需要先和你介绍日志逻辑序列号（log sequence number，LSN）的概念。LSN 是单调递增的，用来对应 redo log 的一个个写入点。每次写入长度为 length 的 redo log， LSN 的值就会加上 length。LSN 也会写到 InnoDB 的数据页中，来确保数据页不会被多次执行重复的 redo log。关 于 LSN 和 redo log、checkpoint 的关系，我会在后面的文章中详细展开。如图 3 所示，是三个并发事务 (trx1, trx2, trx3) 在 prepare 阶段，都写完 redo log buffer，持久化到磁盘的过程，对应的 LSN 分别是 50、120 和 160。 ![](D:\\Java\\blog\\source_posts\\MySQL45讲\\redo log组提交.png) 从图中可以看到， trx1是第一个到达的，会被选择这组的leader； 等trx1要开始写盘时，这个组里面已经有三个事务，这时候LSN也变成了160； trx1去写盘时，带的就是LSN=160，因此等trx1返回时，所有LSN小于等于160的redo log，都已经被持久化到磁盘； 这时候trx2和trx3就可以直接返回了。 一次组提交中，组员越多，节约磁盘IOPS的效果越好。但如果只有单线程压测，那就只能老老实实地一个事务对应一次持久化操作了。 WAL机制主要得益于两个方面： redo log和binlog都是顺序写，磁盘的顺序写比随机写速度要快； 组提交机制，可以大幅度降低磁盘的IOPS消耗。 日志相关问题问题1：执行一个update语句以后，我再去执行hexdump命令直接查看ibd文件内容，为什么没有看到数据有改变呢？ 回答：这可能是因为WAL机制的原因。update语句执行完成后，InnoDB只保证写完了redo log、内存，可能还没来得及将数据写到磁盘。 问题2：为什么binlog cache是每个线程自己维护的，而redo log buffer是全局共用的？ 回答：MySQL这么设计的主要原因是，binlog是不能“被打断的”。一个事务的binlog必须连续写，因此要整个事务完成后，再一起写到文件里。 而redo log并没有这个要求，中间有生成的日志可以写到redo log buffer中，redo log buffer中的内容还能“撘便车”，其他事务提交的时候可以被一起写到磁盘中。 问题3：事务执行期间，还没到提交阶段，如果发生crash的话，redo log肯定丢了，这会不会导致主备不一致呢？ 回答：不会，因为这时候binlog也还在binlog cache里，没发给备库。crash以后redo log和binlog都没有了，从业务角度看这个事务也没有提交，所以数据是一致的。 问题4：如果binlog写完盘以后发生crash，这时候还没给客户端答复就重启了。等客户端再重连进来，发现事务已经提交成功了，这是不是bug？ 回答：不是，例如整个事务都提交成功了，redo log commit完成了，备库也收到binlog并执行了。但是主库和客户端网络断开了，导致事务成功的包返回不回去，这时候客户端也会收到“网络断开”的异常。这种也只能算是事务成功的，不能认为是bug。 实际上数据库的crash-safe保证的是： 如果客户端收到事务成功的消息，事务就一定持久化了； 如果客户端收到事务失败（比如主键冲突、回滚等）的消息，事务就一定失败了； 如果客户端收到“执行异常”的消息，应用需要重连后通过查询当前状态来继续后续的逻辑。此时数据库只需要保证内部（数据和日志之间，主库和备库之间）一致就可以了。 思考题“如果一个数据库是被客户端的压力打满导致无法响应的，重启数据 库是没用的。”，说明他很好地思考了。 这个问题是因为重启之后，业务请求还会再发。而且由于是重启，buffer pool 被清 空，可能会导致语句执行得更慢 有时候一个表上会出现多个单字段索引（而且往往这是 因为运维工程师对索引原理不够清晰做的设计），这样就可能出现优化器选择索引合并 算法的现象。但实际上，索引合并算法的效率并不好。而通过将其中的一个索引改成联 合索引的方法，是一个很好的应对方案。 客户端程序的连接器，连接完成后会做一 些诸如 show columns 的操作，在短连接模式下这个影响就非常大了。 这个提醒我们，在 review 项目的时候，不止要 review 我们自己业务的代 码，也要 review 连接器的行为。一般做法就是在测试环境，把 general_log 打开，用业务行为触发连接，然后通过 general log 分析连接 器的行为。 需要把线上生产库设置成“非双1”的场景： 业务高峰期。一般如果有预知的高峰期，DBA 会有预案，把主库设置成“非双 1”。 备库延迟，为了让备库尽快赶上主库。 用备份恢复主库的副本，应用 binlog 的过程，这个跟上一种场景类似。 批量导入数据的时候。 一般情况下，把生产库改成“非双 1”配置，是设置 innodb_flush_logs_at_trx_commit=2、sync_binlog=1000。 二十四、MySQL是怎么保证主备一致的？MySQL几乎 所有的高可用架构，都直接依赖于 binlog。虽然这些高可用架构已经呈现出越来越复杂的 趋势，但都是从基本的一主一备演化过来的。 MySQL主备的基本原理建议把备库设置成只读（readonly）模式。这样做有以下几个考虑： 有时候一些运营类的查询语句会被放到备库上去查，设置为只读可以防止误操作； 防止切换逻辑有bug，比如切换过程中出现双写，造成主备不一致； 可以用readonly状态，来判断节点的角色。 readonly设置对超级（super）权限用户是无效的，而用于同步更新的线程，就拥有超级权限。所以能够跟主库保持同步更新。 一个事务日志同步的完整过程是这样的： 在备库 B 上通过 change master 命令，设置主库 A 的 IP、端口、用户名、密码，以及 要从哪个位置开始请求 binlog，这个位置包含文件名和日志偏移量。 在备库 B 上执行 start slave 命令，这时候备库会启动两个线程， io_thread 和 sql_thread。其中 io_thread 负责与主库建立连接。 主库 A 校验完用户名、密码后，开始按照备库 B 传过来的位置，从本地读取 binlog， 发给 B。 备库 B 拿到 binlog 后，写到本地文件，称为中转日志（relay log）。 sql_thread 读取中转日志，解析出日志里的命令，并执行。 由于多线程复制方案的引入，sql_thread演化成为了多个线程。 binlog的三种格式对比statement、row、mixed（前两种格式的混合） 由于 statement 格式下，记录到 binlog 里的是语句原文，因此可能会出现这样一种情 况：在主库执行这条 SQL 语句的时候，用的是索引 a；而在备库执行这条 SQL 语句的时 候，却使用了索引 t_modified。因此，MySQL 认为这样写是有风险的。 row格式的binlog里没有SQL语句的原文，而是替换成了两个event：Table_map和Delete_rows. Table_map event,用于说明接下来要操作的表是test库的表t； Delete_rows event，用于定义删除的行为。 rows格式下，通过binlog是看不到详细信息的，还要借助mysqlbinlog工具，用下面这个命令解析和查看binlog中的内容。可以用start_position参数来指定日志解析的起始位置。使用-vv参数是为了把内容都解析出来。 1mysqlbinlog -vv data/master.000001 --start-position=8900; 每个event都有CRC32的值，这是因为我把参数binlog_checksum设置成了CRC32。 binlog_row_image的默认配置是FULL，因此Delete_event里面，包含了删掉的行的所有字段的值。如果把binlog_row_image设置为MINIMAL，则只会记录必要的信息。 为什么会有mixed格式的binlog?因为有些statement格式的binlog可能会导致主备不一致，所以要使用row格式。但row格式的缺点是，很占空间。所以，MySQL就取了个折中方案，也就是有了mixed格式的binlog。（用的不多） mixed格式：MySQL会判断这条SQL语句是否可能引起主备不一致，如果有可能，使用row格式，否则就用statement格式。 现在越来越多的场景要求把MySQL的binlog格式设置成row。这么做的理由有很多，例如：恢复数据。 主要还是用row格式。 循环复制问题 图1为M-S结构，图9为双M结构 实际生产中使用的比较多的是双M结构，即：节点A和B之间总是互为主备关系。这样在切换时就不用再修改主备关系。 但是，双 M 结构还有一个问题需要解决。 业务逻辑在节点 A 上更新了一条语句，然后再把生成的 binlog 发给节点 B，节点 B 执行 完这条更新语句后也会生成 binlog。（我建议你把参数 log_slave_updates 设置为 on， 表示备库执行 relay log 后生成 binlog）。 那么，如果节点 A 同时是节点 B 的备库，相当于又把节点 B 新生成的 binlog 拿过来执行 了一次，然后节点 A 和 B 间，会不断地循环执行这个更新语句，也就是循环复制了。这个 要怎么解决呢？ 用下面的逻辑，来解决两个节点间的循环复制的问题： 规定两个库的 server id 必须不同，如果相同，则它们之间不能设定为主备关系； 一个备库接到 binlog 并在重放的过程中，生成与原 binlog 的 server id 相同的新的 binlog； 每个库在收到从自己的主库发过来的日志后，先判断 server id，如果跟自己的相同，表 示这个日志是自己生成的，就直接丢弃这个日志。 思考题在有server_id的保障下，什么情况下双M结构仍会出现循环复制？ 一种场景是，在一个主库更新事务后，用命令set global server_id=x修改了server_id。等日志再传回来时，发现server_id跟自己的server_id不同，就只能执行了。 另一种场景是，有三个节点时，trx1是在节点B执行的，因此binlog上的server_id就是B，binlog传给节点A，然后A和A‘搭建了双M结构，就会出现循环复制。 这种三节点复制的场景，做数据库迁移时会出现。 二十五、MySQL是怎么保证高可用的？正常情况下，只要主库执行更新生成的所有binlog，都可以传到备库并被正确地执行，备库就能达到跟主库一致的状态，这就是最终一致性。 MySQL要提供高可用能力，只有最终一致性是不够的。 主备延迟与数据同步有关的时间点主要包括以下三个： 主库A执行完成一个事务，写入binlog，我们把这个时刻记为T1； 之后传给备库B，我们把备库B接受完这个binlog的时刻记为T2； 备库B执行完成这个事务，我们把这个时刻记为T3。 所谓主备延迟，就是同一个事务，在备库执行完成的时间和主库执行完成的时间之间的差值，也就是T3~T1。 可以在备库上执行show slave status命令，它的返回结果中会显示seconds_behind_master，用于表示当前备库延迟了多少秒。 主备延迟最直接的表现是，备库消费中转日志（relay log）的速度，比主库生产binlog的速度要慢。 主备延迟的来源第一种可能：有些部署条件下，备库所在及其的性能要比主库所在机器性能差。但更新请求对IOPS的压力，在主库和备库上是无差别的。所以，做这种部署时，一般都会将备库设置为“非双1”的模式。但这种部署现在比较少了，因为很多都是互为主备关系的，所以做对称部署。 第二种可能：备库的压力大。 一些运营后台需要的分析语句，不能影响正常业务，所以只能在备库上跑。由于主库直接影响业务，大家使用起来会比较克制，反而忽视了备库的压力控制。结果就是，备库上的查询耗费了大量的CPU资源，影响了同步速度，造成主备延迟。 这种情况，一般可以这么处理： 一主多从。除了备库外，可以多接几个从库，让这些从库来分担读的压力。 通过binlog输出到外部系统，比如Hadoop这类系统，让外部系统提供统计类查询的能力。 其中，一主多从的方式大都会被采用。 第三种可能：大事务。 因为主库上必须等事务执行完成才会写入binlog，再传给备库。所以，如果一个主库上的语句执行10分钟，那这个事务很可能就会导致从库延迟10分钟。 不要一次性地用delete语句删除太多数据。其实，这就是一个典型的大事务场景。 另一种典型的大事务场景，就是大表DDL。 造成主备延迟还有一个大方向的原因，就是备库的并行复制能力。 可靠性优先策略双 M 结构下，从状态 1 到状态 2 切换的详细过程是这样的： 判断备库 B 现在的 seconds_behind_master，如果小于某个值（比如 5 秒）继续下一 步，否则持续重试这一步； 把主库 A 改成只读状态，即把 readonly 设置为 true； 判断备库 B 的 seconds_behind_master 的值，直到这个值变成 0 为止； 把备库 B 改成可读写状态，也就是把 readonly 设置为 false； 把业务请求切到备库 B。 这个切换流程，一般是由专门的HA系统来完成的，我们暂时称之为可靠性优先流程。 可用性优先策略如果强行把步骤4、5调整到最开始执行，也就是说不等主备数据同步，直接把连接切到备库B，并且让备库B可以读写，那么系统几乎就没有不可用时间了。 这个切换流程，暂时称为可用性优先流程。可能出现数据不一致问题。 使用row格式的binlog时，数据不一致的问题更容易被发现。而使用mixed或者statement格式的binlog时，数据可能悄悄地就不一致了。 大多数情况下，建议使用可靠性优先策略。 在满足数据可靠性的前提下，MySQL高可用系统的可用性，是依赖于主备延迟的。延迟的时间越小，在主库故障的时候，服务恢复需要的时间就越短，可用性就越高。 思考题什么情况下，备库的主备延迟会表现为一个 45 度的线段？ 备库的同步在这段时间完全被堵住了。 产生这种现象典型的场景主要包括两种： ​ 一种是大事务（包括大表DDL、一个事务操作很多行）； ​ 还有一种情况比较隐蔽，就是备库起了一个长事务。 二十六、备库为什么会延迟好几个小时？如果备库执行日志的速度持续低于主库生成日志的速度，那这个延迟就有可能成了小时级别的。 备库并发复制能力 谈到主备的并发复制能力，要关注图中黑色的两个箭头，一个箭头代表了客户端写入主库，另一个箭头代表的是备库上sql_thread执行中转日志（relay log）。如果用箭头的粗细来代表并行度的话，那么真实情况就如图 1 所示，第一个箭头要明显粗于第二个 箭头。 而日志在备库上的执行，就是图中备库上sql_thread更新数据（DATA）的逻辑。如果使用单线程的话，就会导致备库应用日志不够快，造成主备延迟。 在官方的5.6版本之前，MySQL只支持单线程复制，由此在主库并发高、TPS高时就会出现严重的主备延迟问题。 从单线程复制到最新的多线程复制，中间的演化经历了好几个版本。 多线程复制中，coordinator就是原来的sql_thread，不过它现在不再更新数据了，只负责读取中转日志和分发事务给到多个负责更新日志的worker线程，而worker线程的个数是由参数slave_parallel_workers（8~16最好）决定的。 同个事务的多个更新语句不能分给不同的worker来执行。 coordinator在分发时，需要满足以下这两个基本要求： 不能造成更新覆盖，这就要求更新同一行的两个事务，必须被分发到同一个worker中。 同一个事务不能被拆开，必须放到同一个worker中。 二十七、主库出问题了，从库怎么办？大多数的互联网应用场景都是读多写少，因此很可能先会遇到读性能的问题。而在数据库层解决读性能问题，就要设计到接下来两篇文章要讨论的架构：一主多从。 一主多从的设置，一般用于读写分离，主库负责所有的写入和一部分读，其他的读请求则有从库分担。","link":"/2020/09/21/MySQL45%E8%AE%B2/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","link":"/tags/Elasticsearch/"},{"name":"TCP","slug":"TCP","link":"/tags/TCP/"},{"name":"UDP","slug":"UDP","link":"/tags/UDP/"},{"name":"IP协议","slug":"IP协议","link":"/tags/IP%E5%8D%8F%E8%AE%AE/"},{"name":"MVVM","slug":"MVVM","link":"/tags/MVVM/"},{"name":"Vue","slug":"Vue","link":"/tags/Vue/"},{"name":"Java基础","slug":"Java基础","link":"/tags/Java%E5%9F%BA%E7%A1%80/"},{"name":"总结","slug":"总结","link":"/tags/%E6%80%BB%E7%BB%93/"},{"name":"Spring MVC","slug":"Spring-MVC","link":"/tags/Spring-MVC/"},{"name":"VsCode","slug":"VsCode","link":"/tags/VsCode/"},{"name":"API文档自动生成工具","slug":"API文档自动生成工具","link":"/tags/API%E6%96%87%E6%A1%A3%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E5%B7%A5%E5%85%B7/"},{"name":"前后端分离","slug":"前后端分离","link":"/tags/%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB/"},{"name":"Webpack","slug":"Webpack","link":"/tags/Webpack/"},{"name":"mvvm","slug":"mvvm","link":"/tags/mvvm/"},{"name":"数据绑定","slug":"数据绑定","link":"/tags/%E6%95%B0%E6%8D%AE%E7%BB%91%E5%AE%9A/"},{"name":"vue","slug":"vue","link":"/tags/vue/"},{"name":"ajax","slug":"ajax","link":"/tags/ajax/"}],"categories":[{"name":"后端开发","slug":"后端开发","link":"/categories/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/"},{"name":"计算机网络基础","slug":"计算机网络基础","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"},{"name":"Web前端","slug":"Web前端","link":"/categories/Web%E5%89%8D%E7%AB%AF/"},{"name":"Java","slug":"Java","link":"/categories/Java/"},{"name":"IDE","slug":"IDE","link":"/categories/IDE/"},{"name":"前后端分离","slug":"前后端分离","link":"/categories/%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB/"},{"name":"Vue","slug":"Vue","link":"/categories/Vue/"}]}